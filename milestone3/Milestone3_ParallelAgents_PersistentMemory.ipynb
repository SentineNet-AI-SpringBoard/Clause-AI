{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9242eb7",
   "metadata": {},
   "source": [
    "# Milestone 3 — Parallel Agents + Persistent Agent Memory (Pinecone)\n",
    "\n",
    "\n",
    "\n",
    "This notebook implements Milestone 3:\n",
    "\n",
    "- Run **sequential vs parallel** (async) agent pipelines and compare runtime.\n",
    "\n",
    "- **Persist agent outputs** into Pinecone as “agent memory” vectors with metadata (`contract_id`, `agent_type`, `timestamp`).\n",
    "\n",
    "- **Recall stored memory** later (filter by `contract_id` and optionally `agent_type`) to answer follow-ups **without rerunning** the agents.\n",
    "\n",
    "\n",
    "\n",
    "Prereqs:\n",
    "\n",
    "- Pinecone index already populated with contract chunk vectors (Milestone 2).\n",
    "\n",
    "- Environment variable `PINECONE_API_KEY` set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33782173",
   "metadata": {},
   "source": [
    "## 1) Project / Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c984620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 19:22:17,185 | INFO | ROOT=C:\\Users\\LENOVO\\OneDrive\\Dokumen\\legal contracts eda\\milestone3\n",
      "2026-01-09 19:22:17,185 | INFO | ARTIFACTS_DIR=C:\\Users\\LENOVO\\OneDrive\\Dokumen\\legal contracts eda\\milestone3\\artifacts (exists=False)\n",
      "2026-01-09 19:22:17,185 | INFO | Loaded .env keys: []\n",
      "2026-01-09 19:22:17,185 | INFO | Env guards: TRANSFORMERS_NO_TF=1 USE_TF=0 TRANSFORMERS_NO_FLAX=1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "ARTIFACTS_DIR = ROOT / \"artifacts\"\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# Avoid optional TensorFlow/JAX imports (common Windows DLL issues, and not needed here)\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_TF\", \"1\")\n",
    "os.environ.setdefault(\"USE_TF\", \"0\")\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_FLAX\", \"1\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(\"milestone3\")\n",
    "\n",
    "def utc_now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def load_env_file(path: Path) -> Dict[str, str]:\n",
    "    \"\"\"Minimal .env loader (no extra deps).\n",
    "\n",
    "    - Skips blanks and comments\n",
    "    - Supports KEY=VALUE\n",
    "    - Strips surrounding quotes\n",
    "    - Does NOT override already-set env vars\n",
    "    \"\"\"\n",
    "    loaded: Dict[str, str] = {}\n",
    "    if not path.exists():\n",
    "        return loaded\n",
    "\n",
    "    for raw_line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        line = raw_line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        if \"=\" not in line:\n",
    "            continue\n",
    "        key, value = line.split(\"=\", 1)\n",
    "        key = key.strip()\n",
    "        value = value.strip().strip('\"').strip(\"'\")\n",
    "        if not key:\n",
    "            continue\n",
    "        if os.getenv(key) is None:\n",
    "            os.environ[key] = value\n",
    "            loaded[key] = value\n",
    "    return loaded\n",
    "\n",
    "# Auto-load root .env if present\n",
    "env_loaded = load_env_file(ROOT / \".env\")\n",
    "\n",
    "logger.info(f\"ROOT={ROOT}\")\n",
    "logger.info(f\"ARTIFACTS_DIR={ARTIFACTS_DIR} (exists={ARTIFACTS_DIR.exists()})\")\n",
    "logger.info(f\"Loaded .env keys: {sorted(env_loaded.keys())}\")\n",
    "logger.info(\"Env guards: TRANSFORMERS_NO_TF=%s USE_TF=%s TRANSFORMERS_NO_FLAX=%s\", os.getenv(\"TRANSFORMERS_NO_TF\"), os.getenv(\"USE_TF\"), os.getenv(\"TRANSFORMERS_NO_FLAX\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edbca265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared agent list used across the notebook\n",
    "AGENT_TYPES = [\"legal_agent\", \"compliance_agent\", \"finance_agent\", \"operations_agent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a976547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: c:\\Users\\LENOVO\\anaconda3\\python.exe\n",
      "Before:\n",
      "- huggingface-hub: 0.36.0\n",
      "- transformers:    4.57.3\n",
      "- sentence-transformers: 5.2.0\n",
      "- tensorflow:      2.19.0\n",
      "- tensorflow-intel: 2.18.0\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.36.0)\n",
      "Requirement already satisfied: transformers>=4.40.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.57.3)\n",
      "Requirement already satisfied: sentence-transformers>=2.7.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers>=4.40.0) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers>=4.40.0) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers>=4.40.0) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers>=4.40.0) (0.5.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.7.0) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.7.0) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.7.0) (1.13.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.7.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.7.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.7.0) (69.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.7.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.24.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.7.0) (2.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.24.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.24.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.24.0) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.24.0) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.7.0) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.7.0) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "After:\n",
      "- huggingface-hub: 0.36.0\n",
      "- transformers:    4.57.3\n",
      "- sentence-transformers: 5.2.0\n",
      "- tensorflow:      2.19.0\n",
      "- tensorflow-intel: 2.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Dependency repair for this notebook kernel (run once if imports fail)\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import importlib.metadata as _md  # py3.8+\n",
    "except Exception:  # pragma: no cover\n",
    "    _md = None\n",
    "\n",
    "def _v(pkg: str) -> str:\n",
    "    if _md is None:\n",
    "        return \"(unknown)\"\n",
    "    try:\n",
    "        return _md.version(pkg)\n",
    "    except Exception:\n",
    "        return \"(not installed)\"\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Before:\")\n",
    "print(\"- huggingface-hub:\", _v(\"huggingface-hub\"))\n",
    "print(\"- transformers:   \", _v(\"transformers\"))\n",
    "print(\"- sentence-transformers:\", _v(\"sentence-transformers\"))\n",
    "print(\"- tensorflow:     \", _v(\"tensorflow\"))\n",
    "print(\"- tensorflow-intel:\", _v(\"tensorflow-intel\"))\n",
    "\n",
    "# IMPORTANT: %pip installs into the currently-running Jupyter kernel environment.\n",
    "# If you still see the same import error after this, restart the kernel and rerun from Cell 3.\n",
    "%pip install -U \"huggingface-hub>=0.24.0,<1.0\" \"transformers>=4.40.0\" \"sentence-transformers>=2.7.0\"\n",
    "\n",
    "# If you see: \"Failed to load the native TensorFlow runtime\" on Windows, TensorFlow is installed but broken.\n",
    "# Sentence-transformers does not require TensorFlow for embeddings, so removing it is safe for this notebook:\n",
    "# %pip uninstall -y tensorflow tensorflow-intel\n",
    "\n",
    "print(\"\\nAfter:\")\n",
    "print(\"- huggingface-hub:\", _v(\"huggingface-hub\"))\n",
    "print(\"- transformers:   \", _v(\"transformers\"))\n",
    "print(\"- sentence-transformers:\", _v(\"sentence-transformers\"))\n",
    "print(\"- tensorflow:     \", _v(\"tensorflow\"))\n",
    "print(\"- tensorflow-intel:\", _v(\"tensorflow-intel\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae23b0b",
   "metadata": {},
   "source": [
    "## 2) Connect to Pinecone + Load Embeddings Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11a75061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 19:22:20,086 | INFO | Connected to Pinecone index 'cuad-index' via pinecone.Pinecone\n",
      "2026-01-09 19:22:20,087 | INFO | Loading embedding model: all-MiniLM-L6-v2\n",
      "2026-01-09 19:22:20,087 | INFO | Use pytorch device_name: cpu\n",
      "2026-01-09 19:22:20,087 | INFO | Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Pinecone connection (supports both newer and older SDK styles)\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "# Ensure optional TensorFlow/JAX stacks are not used (avoids Windows DLL issues)\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_TF\", \"1\")\n",
    "os.environ.setdefault(\"USE_TF\", \"0\")\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_FLAX\", \"1\")\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "INDEX_NAME = os.getenv(\"PINECONE_INDEX\", \"cuad-index\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "\n",
    "# If the env var isn't set, allow interactive entry (common in notebooks)\n",
    "if not PINECONE_API_KEY:\n",
    "    PINECONE_API_KEY = getpass(\"Enter PINECONE_API_KEY (input hidden): \")\n",
    "    PINECONE_API_KEY = (PINECONE_API_KEY or \"\").strip()\n",
    "    if not PINECONE_API_KEY:\n",
    "        raise RuntimeError(\n",
    "            \"Missing PINECONE_API_KEY. Set it as an environment variable, or re-run and enter it when prompted.\"\n",
    "        )\n",
    "    os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "\n",
    "index = None\n",
    "try:\n",
    "    # Newer SDK\n",
    "    from pinecone import Pinecone\n",
    "\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    index = pc.Index(INDEX_NAME)\n",
    "    logger.info(f\"Connected to Pinecone index '{INDEX_NAME}' via pinecone.Pinecone\")\n",
    "except Exception as e_new:\n",
    "    try:\n",
    "        # Older SDK\n",
    "        import pinecone\n",
    "\n",
    "        if not PINECONE_ENV:\n",
    "            raise RuntimeError(\n",
    "                \"Using legacy pinecone SDK requires PINECONE_ENV. \"\n",
    "                \"Set env var PINECONE_ENV (e.g., 'us-east1-gcp'), then re-run.\"\n",
    "            )\n",
    "        pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "        index = pinecone.Index(INDEX_NAME)\n",
    "        logger.info(f\"Connected to Pinecone index '{INDEX_NAME}' via pinecone.init\")\n",
    "    except Exception as e_old:\n",
    "        raise RuntimeError(f\"Failed to connect to Pinecone: new={e_new} old={e_old}\")\n",
    "\n",
    "# Embedding model\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception as e:\n",
    "    msg = str(e)\n",
    "    if \"Failed to load the native TensorFlow runtime\" in msg or \"_pywrap_tensorflow_internal\" in msg:\n",
    "        raise RuntimeError(\n",
    "            \"TensorFlow is installed but failing to load native DLLs on this machine.\\n\\n\"\n",
    "            \"Sentence-transformers does not require TensorFlow for embeddings. Fix by removing TensorFlow from this env:\\n\"\n",
    "            \"  pip uninstall -y tensorflow tensorflow-intel\\n\\n\"\n",
    "            \"Then restart the kernel and rerun from Section 1.\"\n",
    "        ) from e\n",
    "    if \"huggingface-hub\" in msg and \"Try:\" in msg:\n",
    "        raise RuntimeError(\n",
    "            \"Dependency mismatch while importing sentence-transformers.\\n\\n\"\n",
    "            \"Your environment has an older huggingface-hub that is incompatible with transformers.\\n\\n\"\n",
    "            \"Fix (pip):\\n\"\n",
    "            \"  pip install -U huggingface-hub transformers sentence-transformers\\n\\n\"\n",
    "            \"Fix (conda-forge):\\n\"\n",
    "            \"  conda install -c conda-forge huggingface-hub transformers sentence-transformers\\n\\n\"\n",
    "            \"Then restart the kernel and rerun from Section 1.\"\n",
    "        ) from e\n",
    "    raise RuntimeError(\n",
    "        \"Failed to import sentence-transformers.\\n\\n\"\n",
    "        \"Try:\\n\"\n",
    "        \"  pip install -U sentence-transformers transformers huggingface-hub\\n\\n\"\n",
    "        \"Then restart the kernel and rerun from Section 1.\\n\\n\"\n",
    "        f\"Original error: {type(e).__name__}: {e}\"\n",
    "    ) from e\n",
    "\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"all-MiniLM-L6-v2\")\n",
    "CACHE_DIR = ROOT / \"models_cache\" / \"hub\"\n",
    "logger.info(f\"Loading embedding model: {EMBEDDING_MODEL}\")\n",
    "model = SentenceTransformer(EMBEDDING_MODEL, cache_folder=str(CACHE_DIR))\n",
    "\n",
    "def embed_query(text: str) -> List[float]:\n",
    "    vec = model.encode([text], convert_to_numpy=True)[0]\n",
    "    return vec.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47aa598",
   "metadata": {},
   "source": [
    "## 3) Agent Pipelines (Retrieval-First) + Timing (Sequential vs Parallel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a19e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_TYPES = [\"legal_agent\", \"compliance_agent\", \"finance_agent\", \"operations_agent\"]\n",
    "\n",
    "\n",
    "\n",
    "AGENT_QUERIES: Dict[str, List[str]] = {\n",
    "\n",
    "    \"legal_agent\": [\n",
    "\n",
    "        \"What are the termination clauses and conditions?\",\n",
    "\n",
    "        \"What happens in case of breach of contract?\",\n",
    "\n",
    "        \"What are the confidentiality and non-disclosure obligations?\",\n",
    "\n",
    "        \"What are the indemnification and hold harmless obligations?\",\n",
    "\n",
    "    ],\n",
    "\n",
    "    \"compliance_agent\": [\n",
    "\n",
    "        \"What are the data protection and privacy obligations?\",\n",
    "\n",
    "        \"What regulatory requirements must be followed?\",\n",
    "\n",
    "        \"What are the audit and reporting requirements?\",\n",
    "\n",
    "        \"What are the data retention and deletion obligations?\",\n",
    "\n",
    "        \"What are the breach notification and incident reporting requirements?\",\n",
    "\n",
    "        \"What security audit or certification requirements exist (SOC2/ISO/HIPAA)?\",\n",
    "\n",
    "    ],\n",
    "\n",
    "    \"finance_agent\": [\n",
    "\n",
    "        \"What are the payment terms and conditions?\",\n",
    "\n",
    "        \"What are the fees, invoices, and billing requirements?\",\n",
    "\n",
    "        \"What are the penalties and late fees for non-payment?\",\n",
    "\n",
    "        \"What are the interest charges or interest rate for late payment?\",\n",
    "\n",
    "        \"What is the financial liability and indemnification?\",\n",
    "\n",
    "    ],\n",
    "\n",
    "    \"operations_agent\": [\n",
    "\n",
    "        \"What are the deliverables and project outputs?\",\n",
    "\n",
    "        \"What are the timelines and milestones for delivery?\",\n",
    "\n",
    "        \"What are the service level agreements (SLAs)?\",\n",
    "\n",
    "        \"What are the performance standards and obligations?\",\n",
    "\n",
    "        \"What are the operational requirements and responsibilities?\",\n",
    "\n",
    "        \"What are the uptime commitments, uptime guarantees, and service credits?\",\n",
    "\n",
    "    ],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pinecone_query(\n",
    "\n",
    "    *,\n",
    "\n",
    "    query: str,\n",
    "\n",
    "    top_k: int = 5,\n",
    "\n",
    "    namespace: Optional[str] = None,\n",
    "\n",
    "    metadata_filter: Optional[Dict[str, Any]] = None,\n",
    "\n",
    ") -> Any:\n",
    "\n",
    "    qvec = embed_query(query)\n",
    "\n",
    "    kwargs: Dict[str, Any] = {\n",
    "\n",
    "        \"vector\": qvec,\n",
    "\n",
    "        \"top_k\": top_k,\n",
    "\n",
    "        \"include_metadata\": True,\n",
    "\n",
    "    }\n",
    "\n",
    "    if namespace is not None:\n",
    "\n",
    "        kwargs[\"namespace\"] = namespace\n",
    "\n",
    "    if metadata_filter is not None:\n",
    "\n",
    "        kwargs[\"filter\"] = metadata_filter\n",
    "\n",
    "    return index.query(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _extract_matches(resp: Any) -> List[Dict[str, Any]]:\n",
    "\n",
    "    matches = getattr(resp, \"matches\", None)\n",
    "\n",
    "    if matches is None and isinstance(resp, dict):\n",
    "\n",
    "        matches = resp.get(\"matches\")\n",
    "\n",
    "    if not matches:\n",
    "\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "\n",
    "    for m in matches:\n",
    "\n",
    "        md = getattr(m, \"metadata\", None)\n",
    "\n",
    "        score = getattr(m, \"score\", None)\n",
    "\n",
    "        if md is None and isinstance(m, dict):\n",
    "\n",
    "            md = m.get(\"metadata\")\n",
    "\n",
    "            score = m.get(\"score\")\n",
    "\n",
    "        out.append({\n",
    "\n",
    "            \"score\": float(score) if score is not None else None,\n",
    "\n",
    "            \"metadata\": md or {},\n",
    "\n",
    "        })\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _confidence_from_matches(matches: List[Dict[str, Any]]) -> Optional[float]:\n",
    "\n",
    "    scores = [m.get(\"score\") for m in matches if isinstance(m.get(\"score\"), (int, float))]\n",
    "\n",
    "    if not scores:\n",
    "\n",
    "        return None\n",
    "\n",
    "    return float(sum(scores) / len(scores))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_agent_pipeline(\n",
    "\n",
    "    *,\n",
    "\n",
    "    agent_type: str,\n",
    "\n",
    "    question: str,\n",
    "\n",
    "    contract_id: str,\n",
    "\n",
    "    top_k_per_query: int = 5,\n",
    "\n",
    "    chunks_namespace: Optional[str] = None,\n",
    "\n",
    "    filter_chunks_by_contract_id: bool = False,\n",
    "\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    if agent_type not in AGENT_QUERIES:\n",
    "\n",
    "        raise ValueError(f\"Unknown agent_type: {agent_type}\")\n",
    "\n",
    "\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "\n",
    "\n",
    "    # Only enable this if your chunk vectors' metadata includes: {\"contract_id\": \"...\"}\n",
    "\n",
    "    md_filter = (\n",
    "\n",
    "        {\"contract_id\": {\"$eq\": contract_id}}\n",
    "\n",
    "        if filter_chunks_by_contract_id\n",
    "\n",
    "        else None\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    all_matches: List[Dict[str, Any]] = []\n",
    "\n",
    "    per_query: List[Dict[str, Any]] = []\n",
    "\n",
    "    for q in AGENT_QUERIES[agent_type]:\n",
    "\n",
    "        resp = pinecone_query(query=q, top_k=top_k_per_query, namespace=chunks_namespace, metadata_filter=md_filter)\n",
    "\n",
    "        matches = _extract_matches(resp)\n",
    "\n",
    "        per_query.append({\"query\": q, \"matches\": matches})\n",
    "\n",
    "        all_matches.extend(matches)\n",
    "\n",
    "\n",
    "\n",
    "    confidence = _confidence_from_matches(all_matches)\n",
    "\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "\n",
    "        \"agent_type\": agent_type,\n",
    "\n",
    "        \"contract_id\": contract_id,\n",
    "\n",
    "        \"question\": question,\n",
    "\n",
    "        \"timestamp\": utc_now_iso(),\n",
    "\n",
    "        \"elapsed_seconds\": elapsed,\n",
    "\n",
    "        \"confidence\": confidence,\n",
    "\n",
    "        \"retrieval\": {\n",
    "\n",
    "            \"top_k_per_query\": top_k_per_query,\n",
    "\n",
    "            \"filter_chunks_by_contract_id\": filter_chunks_by_contract_id,\n",
    "\n",
    "            \"per_query\": per_query,\n",
    "\n",
    "        },\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_sequential(\n",
    "\n",
    "    *,\n",
    "\n",
    "    question: str,\n",
    "\n",
    "    contract_id: str,\n",
    "\n",
    "    agents: List[str] = AGENT_TYPES,\n",
    "\n",
    "    filter_chunks_by_contract_id: bool = False,\n",
    "\n",
    ") -> Tuple[Dict[str, Any], float]:\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    out: Dict[str, Any] = {}\n",
    "\n",
    "    for a in agents:\n",
    "\n",
    "        out[a] = run_agent_pipeline(\n",
    "\n",
    "            agent_type=a,\n",
    "\n",
    "            question=question,\n",
    "\n",
    "            contract_id=contract_id,\n",
    "\n",
    "            filter_chunks_by_contract_id=filter_chunks_by_contract_id,\n",
    "\n",
    "        )\n",
    "\n",
    "    return out, time.perf_counter() - t0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "async def run_parallel(\n",
    "\n",
    "    *,\n",
    "\n",
    "    question: str,\n",
    "\n",
    "    contract_id: str,\n",
    "\n",
    "    agents: List[str] = AGENT_TYPES,\n",
    "\n",
    "    filter_chunks_by_contract_id: bool = False,\n",
    "\n",
    ") -> Tuple[Dict[str, Any], float]:\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    tasks = [\n",
    "\n",
    "        asyncio.to_thread(\n",
    "\n",
    "            run_agent_pipeline,\n",
    "\n",
    "            agent_type=a,\n",
    "\n",
    "            question=question,\n",
    "\n",
    "            contract_id=contract_id,\n",
    "\n",
    "            filter_chunks_by_contract_id=filter_chunks_by_contract_id,\n",
    "\n",
    "        )\n",
    "\n",
    "        for a in agents\n",
    "\n",
    "    ]\n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    out = {r[\"agent_type\"]: r for r in results}\n",
    "\n",
    "    return out, time.perf_counter() - t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77c827a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce521651c3294c6ea7d66e27dd043a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587a7ae68d31462aa991ca50cc86d707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f7be6a76e44932a314effff1c8ef02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d094c4d54d9947d49f2ab813f26cb7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b87ac021164ccebccf0323af539bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2901b13c95394b1b9acb02c0e74af1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59646981a47946549fb3644b374e8b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3837abfcdeb6459c9044671b4343e965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03dd1b376b9d41028dd412af6bcbb822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5512a90bb37f4bdcb6b9596fb0a3bed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a872a9f95814a989a28945f05185c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4754f62a8c744b5a720d975d23ef765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828df9bf71f749b9ac4b33f3539adadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad4c2ff4b8b4dd8a9d838f078cd5137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fea7e4ae2274b7b9c3753a36ff3ea5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee367b9b930463aab72f36c06b9b06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9f0034a7904994988f3439b5928ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06e435abbce4bf6baada1f10f7e5f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549db55491614a8bb46bdc81f20154c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ac7ec727714959b20b566453e3a117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4534c2f0b1274fe5834d14e5266769e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637293528ec248a5922aa2a1c400403a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03285ab29e544b449674a8f95e338ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae27752746d4c0b82774bdef977cd9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f453c80165d481f8df35505e3950d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4afacf6dd44cf78d290b8ce1760857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a96c8ff05c6497f8dad20789737e0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373b5ea03ab848c19d1c761b6a9eb67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae5c2e3ac814d75a3b3fa493f49ecfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e102b2bff3c640b5a3f619a7298f17a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276e8264408949e7bc1496adc3446cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f218ab8874148a2b3d0c611089b0e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a941225eb324b1388da94e950dd1200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526f810f3f0744c3994629e665d54021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a325d9ac62ac48ec9f6120890969933c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6303e734f484a018c0c63754c3be25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a77739a80747eead7fc2ec58b6f93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d40c848868144a0898045218bc97cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253bda9e2446488ebdc48671e127405d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7811dcf783af4872a1dfc2d57109d81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dcd2a68b254468888ea024e92022f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02aee4f43e794e32962e8c35b95417c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential seconds: 9.716\n",
      "Parallel seconds:   3.344\n",
      "\n",
      "Per-agent confidence (parallel):\n",
      "- legal_agent: 0.6607\n",
      "- compliance_agent: 0.5007\n",
      "- finance_agent: 0.5432\n",
      "- operations_agent: 0.4704\n"
     ]
    }
   ],
   "source": [
    "# Configure your run\n",
    "\n",
    "# - CONTRACT_ID can be any stable identifier you choose (used for memory persistence/recall).\n",
    "\n",
    "# - If your chunk vectors include contract_id in metadata, set FILTER_CHUNKS_BY_CONTRACT_ID=True.\n",
    "\n",
    "CONTRACT_ID = os.getenv(\"CONTRACT_ID\", \"demo_contract\")\n",
    "\n",
    "QUESTION = \"What are the payment terms, audit requirements, and uptime commitments?\"\n",
    "\n",
    "FILTER_CHUNKS_BY_CONTRACT_ID = False\n",
    "\n",
    "\n",
    "\n",
    "seq_out, seq_s = run_sequential(\n",
    "\n",
    "    question=QUESTION,\n",
    "\n",
    "    contract_id=CONTRACT_ID,\n",
    "\n",
    "    filter_chunks_by_contract_id=FILTER_CHUNKS_BY_CONTRACT_ID,\n",
    "\n",
    ")\n",
    "\n",
    "par_out, par_s = await run_parallel(\n",
    "\n",
    "    question=QUESTION,\n",
    "\n",
    "    contract_id=CONTRACT_ID,\n",
    "\n",
    "    filter_chunks_by_contract_id=FILTER_CHUNKS_BY_CONTRACT_ID,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Sequential seconds:\", round(seq_s, 3))\n",
    "\n",
    "print(\"Parallel seconds:  \", round(par_s, 3))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nPer-agent confidence (parallel):\")\n",
    "\n",
    "for a in AGENT_TYPES:\n",
    "\n",
    "    conf = par_out[a].get(\"confidence\")\n",
    "\n",
    "    print(f\"- {a}: {None if conf is None else round(conf, 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb88b18",
   "metadata": {},
   "source": [
    "## 4) Persist Agent Outputs as Vector Memory (Pinecone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6434a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_MEMORY_NAMESPACE = \"agent_memory\"\n",
    "\n",
    "AGENT_MEMORY_RECORD_TYPE = \"agent_memory\"\n",
    "\n",
    "\n",
    "\n",
    "def safe_json_dumps(obj: Any, max_chars: int = 6000) -> str:\n",
    "    \"\"\"Serialize to JSON for storage in metadata.\n",
    "\n",
    "    Important: If we truncate, we keep the returned string as VALID JSON so it can be json.loads()'d later.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = json.dumps(obj, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        s = str(obj)\n",
    "    if len(s) <= max_chars:\n",
    "        return s\n",
    "    # Wrap a preview in a valid JSON object to avoid broken/partial JSON strings\n",
    "    preview = s[: max_chars - 200]\n",
    "    wrapper = {\"_truncated\": True, \"preview\": preview, \"chars\": len(s)}\n",
    "    return json.dumps(wrapper, ensure_ascii=False)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentMemoryRecord:\n",
    "    contract_id: str\n",
    "    agent_type: str\n",
    "    timestamp: str\n",
    "    question: str\n",
    "    output: Any\n",
    "\n",
    "    def to_text(self) -> str:\n",
    "        return (\n",
    "            f\"contract_id: {self.contract_id}\\n\"\n",
    "            f\"agent_type: {self.agent_type}\\n\"\n",
    "            f\"timestamp: {self.timestamp}\\n\"\n",
    "            f\"question: {self.question}\\n\\n\"\n",
    "            f\"output_json: {safe_json_dumps(self.output)}\\n\"\n",
    "        )\n",
    "\n",
    "    def to_metadata(self) -> Dict[str, Any]:\n",
    "        md: Dict[str, Any] = {\n",
    "            \"record_type\": AGENT_MEMORY_RECORD_TYPE,\n",
    "            \"contract_id\": self.contract_id,\n",
    "            \"agent_type\": self.agent_type,\n",
    "            # Alias to match common examples\n",
    "            \"agent\": self.agent_type,\n",
    "            \"timestamp\": self.timestamp,\n",
    "            \"question\": self.question[:1000],\n",
    "            \"output_json\": safe_json_dumps(self.output, 6000),\n",
    "        }\n",
    "\n",
    "        # If output contains a risk_level or confidence, store explicitly for filtering / reporting.\n",
    "        if isinstance(self.output, dict):\n",
    "            rl = self.output.get(\"risk_level\")\n",
    "            if isinstance(rl, str) and rl.strip():\n",
    "                md[\"risk_level\"] = rl.strip().lower()\n",
    "            conf = self.output.get(\"confidence\")\n",
    "            if isinstance(conf, (int, float)):\n",
    "                md[\"confidence\"] = float(conf)\n",
    "\n",
    "        return md\n",
    "\n",
    "\n",
    "def persist_agent_memory(*, records: List[AgentMemoryRecord], namespace: str = AGENT_MEMORY_NAMESPACE) -> List[str]:\n",
    "    vectors = []\n",
    "    ids: List[str] = []\n",
    "    for r in records:\n",
    "        text = r.to_text()\n",
    "        vec = embed_query(text)\n",
    "        vid = f\"{r.contract_id}:{r.agent_type}:{r.timestamp}:{uuid.uuid4().hex}\"\n",
    "        ids.append(vid)\n",
    "        vectors.append({\"id\": vid, \"values\": vec, \"metadata\": r.to_metadata()})\n",
    "\n",
    "    index.upsert(vectors=vectors, namespace=namespace)\n",
    "    return ids\n",
    "\n",
    "\n",
    "def query_agent_memory(*, query: str, contract_id: str, agent_type: Optional[str] = None, top_k: int = 5, namespace: str = AGENT_MEMORY_NAMESPACE) -> Any:\n",
    "    filt: Dict[str, Any] = {\n",
    "        \"record_type\": {\"$eq\": AGENT_MEMORY_RECORD_TYPE},\n",
    "        \"contract_id\": {\"$eq\": contract_id},\n",
    "    }\n",
    "    if agent_type:\n",
    "        filt[\"agent_type\"] = {\"$eq\": agent_type}\n",
    "\n",
    "    return index.query(\n",
    "        vector=embed_query(query),\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "        namespace=namespace,\n",
    "        filter=filt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f4b725e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fc2f0fd14a4a15b4d5d01461a06ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8260d5d6b8bc4d289b39a382027daa45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181bb9d71c4e4d7c90b936b5fbc9297a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa98b98851e4f8cbfe27a7fedcd9a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 4 agent-memory vectors into namespace 'agent_memory'.\n",
      "Example IDs: ['demo_contract:legal_agent:2026-01-09T13:52:38.517959+00:00:6a8ebe41b4d04600b5e73e6919d81934', 'demo_contract:compliance_agent:2026-01-09T13:52:38.517959+00:00:35723597c5644adcb45da0cfcf076162']\n"
     ]
    }
   ],
   "source": [
    "# Persist the PARALLEL outputs from Section 3\n",
    "\n",
    "records = [\n",
    "\n",
    "    AgentMemoryRecord(\n",
    "\n",
    "        contract_id=CONTRACT_ID,\n",
    "\n",
    "        agent_type=a,\n",
    "\n",
    "        timestamp=utc_now_iso(),\n",
    "\n",
    "        question=QUESTION,\n",
    "\n",
    "        output=par_out[a],\n",
    "\n",
    "    )\n",
    "\n",
    "    for a in AGENT_TYPES\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "ids = persist_agent_memory(records=records)\n",
    "\n",
    "print(f\"Upserted {len(ids)} agent-memory vectors into namespace '{AGENT_MEMORY_NAMESPACE}'.\")\n",
    "\n",
    "print(\"Example IDs:\", ids[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4d04b0",
   "metadata": {},
   "source": [
    "## 5) Recall Stored Agent Memory (No Rerun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb5c5fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7411b4c4174a049f18fa6e3ab43256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966daafedcd04556a4c863da9d8ab0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations memory matches:\n",
      "- score: 0.300049812\n",
      "  ts: 2026-01-08T13:29:46.417164+00:00\n",
      "  question: What are the payment terms, audit requirements, and uptime commitments?\n",
      "- score: 0.287986785\n",
      "  ts: 2026-01-09T13:31:38.825623+00:00\n",
      "  question: What are the payment terms, audit requirements, and uptime commitments?\n",
      "- score: 0.244141608\n",
      "  ts: 2026-01-09T13:47:16.141924+00:00\n",
      "  question: What are the payment terms, audit requirements, and uptime commitments?\n",
      "\n",
      "Finance memory matches:\n",
      "- score: 0.208692566\n",
      "  ts: 2026-01-08T13:29:46.417164+00:00\n",
      "  question: What are the payment terms, audit requirements, and uptime commitments?\n",
      "- score: 0.205164433\n",
      "  ts: 2026-01-09T13:31:38.825623+00:00\n",
      "  question: What are the payment terms, audit requirements, and uptime commitments?\n",
      "- score: 0.16263485\n",
      "  ts: 2026-01-09T13:47:16.141924+00:00\n",
      "  question: What are the payment terms, audit requirements, and uptime commitments?\n"
     ]
    }
   ],
   "source": [
    "# Recall examples (filtered by contract_id and optionally agent_type)\n",
    "\n",
    "recall_ops = query_agent_memory(\n",
    "\n",
    "    query=\"uptime commitments service credits\",\n",
    "\n",
    "    contract_id=CONTRACT_ID,\n",
    "\n",
    "    agent_type=\"operations_agent\",\n",
    "\n",
    "    top_k=3,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "recall_fin = query_agent_memory(\n",
    "\n",
    "    query=\"interest charges late payment\",\n",
    "\n",
    "    contract_id=CONTRACT_ID,\n",
    "\n",
    "    agent_type=\"finance_agent\",\n",
    "\n",
    "    top_k=3,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Operations memory matches:\")\n",
    "\n",
    "for m in getattr(recall_ops, \"matches\", [])[:3]:\n",
    "\n",
    "    print(\"- score:\", getattr(m, \"score\", None))\n",
    "\n",
    "    print(\"  ts:\", (getattr(m, \"metadata\", {}) or {}).get(\"timestamp\"))\n",
    "\n",
    "    print(\"  question:\", (getattr(m, \"metadata\", {}) or {}).get(\"question\"))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nFinance memory matches:\")\n",
    "\n",
    "for m in getattr(recall_fin, \"matches\", [])[:3]:\n",
    "\n",
    "    print(\"- score:\", getattr(m, \"score\", None))\n",
    "\n",
    "    print(\"  ts:\", (getattr(m, \"metadata\", {}) or {}).get(\"timestamp\"))\n",
    "\n",
    "    print(\"  question:\", (getattr(m, \"metadata\", {}) or {}).get(\"question\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb6686",
   "metadata": {},
   "source": [
    "## 6) Cross-Agent Refinement (Memory → Shared Context → Refine → Persist)\n",
    "\n",
    "> Goal: enable one agent to use another agent’s stored output as context, refine risk assessment, and write back the refined result into Pinecone memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b39c613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2390bd1563a484999f0c72b2b776c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c089fc3f0f489badb0d07215fdd8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b6d273718d4b028ffe19ee2cfbb16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020df756863e48d5a3152a1469044aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legal_agent risk: high\n",
      "compliance_agent risk: high\n",
      "finance_agent risk: high\n",
      "operations_agent risk: medium\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "# Fallback if earlier cells were not executed yet\n",
    "AGENTS_FOR_REFINEMENT = globals().get(\"AGENT_TYPES\") or [\n",
    "    \"legal_agent\",\n",
    "    \"compliance_agent\",\n",
    "    \"finance_agent\",\n",
    "    \"operations_agent\",\n",
    "]\n",
    "\n",
    "def _as_utc_aware(dt: datetime) -> datetime:\n",
    "    \"\"\"Normalize datetimes to timezone-aware UTC for safe comparisons.\"\"\"\n",
    "    if dt.tzinfo is None:\n",
    "        return dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def _parse_ts(ts: Optional[str]) -> datetime:\n",
    "    # Always return a timezone-aware UTC datetime to avoid naive/aware comparison errors.\n",
    "    if not ts:\n",
    "        return datetime.min.replace(tzinfo=timezone.utc)\n",
    "    try:\n",
    "        # Handles ISO 8601 like: 2026-01-08T12:34:56.789+00:00 or ...Z\n",
    "        dt = datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "        return _as_utc_aware(dt)\n",
    "    except Exception:\n",
    "        return datetime.min.replace(tzinfo=timezone.utc)\n",
    "\n",
    "def _matches(resp: Any) -> List[Any]:\n",
    "    if isinstance(resp, dict):\n",
    "        return resp.get(\"matches\") or []\n",
    "    return getattr(resp, \"matches\", []) or []\n",
    "\n",
    "def _md(match: Any) -> Dict[str, Any]:\n",
    "    if isinstance(match, dict):\n",
    "        return match.get(\"metadata\") or {}\n",
    "    return getattr(match, \"metadata\", {}) or {}\n",
    "\n",
    "def _infer_risk_from_text(text: str) -> Tuple[str, str]:\n",
    "    t = (text or \"\").lower()\n",
    "    # Very simple heuristic just for milestone demonstration\n",
    "    high_terms = [\"penalt\", \"late fee\", \"interest\", \"termination\", \"breach\", \"indemn\", \"liability\", \"service credit\"]\n",
    "    medium_terms = [\"audit\", \"confidential\", \"privacy\", \"retention\", \"notification\", \"sla\"]\n",
    "\n",
    "    if any(k in t for k in high_terms):\n",
    "        return \"high\", \"Contains high-impact financial/legal terms (heuristic).\"\n",
    "    if any(k in t for k in medium_terms):\n",
    "        return \"medium\", \"Contains standard compliance/operations terms (heuristic).\"\n",
    "    return \"medium\", \"Defaulted to medium (insufficient signal in stored output).\"\n",
    "\n",
    "def fetch_latest_agent_memory(*, contract_id: str, agent_type: str, top_k: int = 10) -> Optional[Dict[str, Any]]:\n",
    "    resp = query_agent_memory(query=f\"{agent_type} risk assessment\", contract_id=contract_id, agent_type=agent_type, top_k=top_k)\n",
    "    best = None\n",
    "    best_ts = datetime.min.replace(tzinfo=timezone.utc)\n",
    "    for m in _matches(resp):\n",
    "        md = _md(m)\n",
    "        ts = _parse_ts(md.get(\"timestamp\"))\n",
    "        if ts > best_ts:\n",
    "            best = md\n",
    "            best_ts = ts\n",
    "    return best\n",
    "\n",
    "# 1) Retrieve latest memory per agent and build shared_context\n",
    "latest_by_agent: Dict[str, Dict[str, Any]] = {}\n",
    "for agent_type in AGENTS_FOR_REFINEMENT:\n",
    "    md = fetch_latest_agent_memory(contract_id=CONTRACT_ID, agent_type=agent_type)\n",
    "    if md is None:\n",
    "        latest_by_agent[agent_type] = {\n",
    "            \"agent\": agent_type,\n",
    "            \"risk_level\": \"unknown\",\n",
    "            \"confidence\": None,\n",
    "            \"timestamp\": None,\n",
    "            \"output_json\": \"\",\n",
    "        }\n",
    "        continue\n",
    "\n",
    "    # Prefer explicit risk_level metadata, else infer from stored output_json text\n",
    "    output_json = md.get(\"output_json\") or \"\"\n",
    "    risk_level = md.get(\"risk_level\")\n",
    "    if not isinstance(risk_level, str) or not risk_level.strip():\n",
    "        risk_level, _ = _infer_risk_from_text(output_json)\n",
    "\n",
    "    # Best-effort confidence from memory metadata\n",
    "    conf = md.get(\"confidence\")\n",
    "    if not isinstance(conf, (int, float)):\n",
    "        conf = None\n",
    "\n",
    "    latest_by_agent[agent_type] = {\n",
    "        \"agent\": md.get(\"agent\") or md.get(\"agent_type\") or agent_type,\n",
    "        \"risk_level\": risk_level,\n",
    "        \"confidence\": float(conf) if isinstance(conf, (int, float)) else None,\n",
    "        \"timestamp\": md.get(\"timestamp\"),\n",
    "        \"output_json\": output_json,\n",
    "    }\n",
    "\n",
    "shared_context = \"\\n\".join([f\"{v['agent']} risk: {v['risk_level']}\" for v in latest_by_agent.values()])\n",
    "print(shared_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5f2a6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"agent_type\": \"compliance_agent\",\n",
      "  \"risk_level\": \"high\",\n",
      "  \"confidence\": 0.54319379216,\n",
      "  \"reason\": \"No escalation: finance risk not high (heuristic).\",\n",
      "  \"based_on\": {\n",
      "    \"shared_context\": \"legal_agent risk: high\\ncompliance_agent risk: high\\nfinance_agent risk: high\\noperations_agent risk: medium\",\n",
      "    \"finance_risk\": \"high\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 2) Let compliance agent read finance output and refine (risk escalation demo)\n",
    "finance = latest_by_agent.get(\"finance_agent\", {})\n",
    "compliance = latest_by_agent.get(\"compliance_agent\", {})\n",
    "\n",
    "finance_risk = (finance.get(\"risk_level\") or \"unknown\").lower()\n",
    "compliance_risk = (compliance.get(\"risk_level\") or \"unknown\").lower()\n",
    "\n",
    "# Best-effort confidence: inherit from latest compliance if available, else finance, else None\n",
    "def _as_float(x: Any) -> Optional[float]:\n",
    "    return float(x) if isinstance(x, (int, float)) else None\n",
    "\n",
    "inherited_confidence = (\n",
    "    _as_float(compliance.get(\"confidence\"))\n",
    "    or _as_float((compliance.get(\"output\") or {}).get(\"confidence\") if isinstance(compliance.get(\"output\"), dict) else None)\n",
    "    or _as_float(finance.get(\"confidence\"))\n",
    "    or _as_float((finance.get(\"output\") or {}).get(\"confidence\") if isinstance(finance.get(\"output\"), dict) else None)\n",
    "    or None\n",
    " )\n",
    "\n",
    "refined_risk = compliance_risk\n",
    "reason = \"No escalation: finance risk not high (heuristic).\"\n",
    "\n",
    "if finance_risk == \"high\" and compliance_risk in {\"low\", \"medium\", \"unknown\"}:\n",
    "    refined_risk = \"high\"\n",
    "    reason = \"Escalated to high because finance risk is high; combined exposure increases compliance risk.\"\n",
    "\n",
    "refined_compliance = {\n",
    "    \"agent_type\": \"compliance_agent\",\n",
    "    \"risk_level\": refined_risk,\n",
    "    \"confidence\": inherited_confidence,\n",
    "    \"reason\": reason,\n",
    "    \"based_on\": {\n",
    "        \"shared_context\": shared_context,\n",
    "        \"finance_risk\": finance_risk,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(json.dumps(refined_compliance, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38406d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b196d041a94c2bb992d099ba6b9c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted refined compliance memory: demo_contract:compliance_agent:2026-01-09T14:01:55.035860+00:00:942d633582c140b7b21d36109d479307\n"
     ]
    }
   ],
   "source": [
    "# 3) Update Compliance memory (persist refined assessment)\n",
    "refined_record = AgentMemoryRecord(\n",
    "    contract_id=CONTRACT_ID,\n",
    "    agent_type=\"compliance_agent\",\n",
    "    timestamp=utc_now_iso(),\n",
    "    question=\"Cross-agent refinement: compliance reads finance output and re-evaluates risk\",\n",
    "    output=refined_compliance,\n",
    ")\n",
    "\n",
    "refined_ids = persist_agent_memory(records=[refined_record])\n",
    "print(\"Upserted refined compliance memory:\", refined_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b540456",
   "metadata": {},
   "source": [
    "## 7) Final Contract-Level JSON Output (Latest Memories → Standard JSON)\n",
    "\n",
    "This section produces a **single standardized JSON** for a contract by:\n",
    "- Pulling the **latest** stored memory per agent from Pinecone\n",
    "- Aggregating **confidence** across agents\n",
    "- Extracting a list of **high-risk clauses** (evidence snippets)\n",
    "- Computing an **overall risk level**\n",
    "- Saving the final JSON to disk (so it can be used outside the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aad44c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d09c2d5e7ad41d48df5fd3f6a8fa3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aba1d57870744188fc3f888b33ad874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6008c6aea54c04b89fe6133a16c656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1fb91fb049404c9838780137375f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\LENOVO\\OneDrive\\Dokumen\\legal contracts eda\\milestone3\\outputs\\final_contract_demo_contract.json\n",
      "overall_risk: high\n",
      "confidence overall_avg: 0.5543624817841666\n",
      "high_risk_clauses: 3\n"
     ]
    }
   ],
   "source": [
    "# Define the final schema + generate the final contract-level JSON from latest Pinecone memories\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "# Where to save outputs (prefer milestone3/outputs if present; otherwise milestone3/artifacts)\n",
    "OUTPUTS_DIR = (ROOT / \"outputs\") if (ROOT / \"outputs\").exists() else (ROOT / \"artifacts\")\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RISK_ORDER = {\"low\": 0, \"medium\": 1, \"high\": 2, \"unknown\": 1}\n",
    "HIGH_RISK_TERMS = [\n",
    "    \"penalt\", \"late fee\", \"interest\", \"termination\", \"breach\", \"indemn\", \"liability\", \"service credit\",\n",
    "    \"audit right\", \"uncapped\", \"limitation of liability\", \"data breach\", \"incident\", \"non-compliance\",\n",
    "    \"security\", \"subprocessor\", \"cross-border\", \"governing law\", \"injunction\",\n",
    "]\n",
    "\n",
    "# Defensive: if this list ever becomes nested (e.g., due to a trailing comma edit), flatten it\n",
    "if HIGH_RISK_TERMS and isinstance(HIGH_RISK_TERMS[0], (list, tuple, set)):\n",
    "    HIGH_RISK_TERMS = [t for group in HIGH_RISK_TERMS for t in group]\n",
    "\n",
    "FINAL_CONTRACT_SCHEMA: Dict[str, Any] = {\n",
    "    \"contract_id\": \"\",\n",
    "    \"legal\": {},\n",
    "    \"compliance\": {},\n",
    "    \"finance\": {},\n",
    "    \"operations\": {},\n",
    "    \"overall_risk\": \"\",\n",
    "    \"confidence\": {\n",
    "        \"per_agent\": {},\n",
    "        \"overall_avg\": None,\n",
    "    },\n",
    "    \"high_risk_clauses\": [],\n",
    "    \"generated_at\": \"\",\n",
    "}\n",
    "\n",
    "def _utc_now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def _as_utc_aware(dt: datetime) -> datetime:\n",
    "    if dt.tzinfo is None:\n",
    "        return dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def _parse_ts(ts: Optional[str]) -> datetime:\n",
    "    if not ts:\n",
    "        return datetime.min.replace(tzinfo=timezone.utc)\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "        return _as_utc_aware(dt)\n",
    "    except Exception:\n",
    "        return datetime.min.replace(tzinfo=timezone.utc)\n",
    "\n",
    "def _matches(resp: Any) -> List[Any]:\n",
    "    if isinstance(resp, dict):\n",
    "        return resp.get(\"matches\") or []\n",
    "    return getattr(resp, \"matches\", []) or []\n",
    "\n",
    "def _md(match: Any) -> Dict[str, Any]:\n",
    "    if isinstance(match, dict):\n",
    "        return match.get(\"metadata\") or {}\n",
    "    return getattr(match, \"metadata\", {}) or {}\n",
    "\n",
    "def _safe_json_loads(s: str) -> Optional[Any]:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _extract_text_from_match_metadata(md: Dict[str, Any]) -> str:\n",
    "    # Try common metadata keys used by chunking pipelines\n",
    "    for key in (\"text\", \"chunk_text\", \"content\", \"clause_text\", \"snippet\", \"page_content\"):\n",
    "        v = md.get(key)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip()\n",
    "    # Fallback: show a compact representation\n",
    "    try:\n",
    "        return json.dumps(md, ensure_ascii=False)[:800]\n",
    "    except Exception:\n",
    "        return str(md)[:800]\n",
    "\n",
    "def _infer_risk_level(output: Any) -> str:\n",
    "    if isinstance(output, dict):\n",
    "        rl = output.get(\"risk_level\")\n",
    "        if isinstance(rl, str) and rl.strip():\n",
    "            return rl.strip().lower()\n",
    "    # fallback heuristic on serialized output\n",
    "    text = json.dumps(output, ensure_ascii=False).lower() if output is not None else \"\"\n",
    "    if any(t in text for t in HIGH_RISK_TERMS):\n",
    "        return \"high\"\n",
    "    return \"medium\"\n",
    "\n",
    "def _extract_term_hits(*, text: str, agent_type: str, max_items: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fallback evidence extraction when we don't have structured retrieval matches available.\"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return []\n",
    "    lower = text.lower()\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    seen_terms = set()\n",
    "    for term in HIGH_RISK_TERMS:\n",
    "        if term in seen_terms:\n",
    "            continue\n",
    "        idx = lower.find(term)\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        seen_terms.add(term)\n",
    "        start = max(0, idx - 120)\n",
    "        end = min(len(text), idx + 160)\n",
    "        snippet = text[start:end].replace(\"\\n\", \" \")\n",
    "        out.append({\n",
    "            \"agent\": agent_type,\n",
    "            \"query\": \"(memory-text-scan)\",\n",
    "            \"score\": None,\n",
    "            \"snippet\": snippet[:800],\n",
    "            \"is_high_risk\": True,\n",
    "            \"matched_term\": term,\n",
    "        })\n",
    "        if len(out) >= max_items:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def get_latest_agent_output(*, contract_id: str, agent_type: str, top_k: int = 10) -> Dict[str, Any]:\n",
    "    \"\"\"Return the latest stored agent output for this contract+agent.\n",
    "\n",
    "    Best-effort confidence behavior: if the latest record has no numeric confidence (common for refinement-only\n",
    "    records), fall back to the newest record in the result set that *does* have confidence.\n",
    "    \"\"\"\n",
    "    resp = query_agent_memory(\n",
    "        query=\"risk\",\n",
    "        contract_id=contract_id,\n",
    "        agent_type=agent_type,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "    matches = _matches(resp)\n",
    "    if not matches:\n",
    "        return {\n",
    "            \"agent_type\": agent_type,\n",
    "            \"timestamp\": None,\n",
    "            \"risk_level\": \"unknown\",\n",
    "            \"confidence\": None,\n",
    "            \"output\": {\"risk_level\": \"unknown\", \"note\": \"No memory found\"},\n",
    "            \"_memory_metadata\": {},\n",
    "        }\n",
    "\n",
    "    # Sort all candidates by timestamp (desc)\n",
    "    ranked: List[Tuple[datetime, Dict[str, Any]]] = []\n",
    "    for m in matches:\n",
    "        md = _md(m)\n",
    "        ranked.append((_parse_ts(md.get(\"timestamp\")), md))\n",
    "    ranked.sort(key=lambda t: t[0], reverse=True)\n",
    "\n",
    "    best_md = ranked[0][1]\n",
    "    # output_json is expected to be JSON; if it was truncated, it should still be valid JSON wrapper\n",
    "    output_raw = best_md.get(\"output_json\") or \"\"\n",
    "    output_obj = _safe_json_loads(output_raw)\n",
    "    if output_obj is None:\n",
    "        output_obj = {\"raw_output_json\": output_raw}\n",
    "\n",
    "    # Prefer explicit risk_level metadata, else infer\n",
    "    risk_level = best_md.get(\"risk_level\")\n",
    "    if not isinstance(risk_level, str) or not risk_level.strip():\n",
    "        risk_level = _infer_risk_level(output_obj)\n",
    "\n",
    "    # 1) First try: explicit confidence from the latest record's metadata\n",
    "    conf: Optional[float] = None\n",
    "    c_latest = best_md.get(\"confidence\")\n",
    "    if isinstance(c_latest, (int, float)):\n",
    "        conf = float(c_latest)\n",
    "    # 2) Second try: confidence embedded in the latest record's output JSON\n",
    "    if conf is None and isinstance(output_obj, dict):\n",
    "        c2 = output_obj.get(\"confidence\")\n",
    "        if isinstance(c2, (int, float)):\n",
    "            conf = float(c2)\n",
    "    # 3) Fallback: newest record (by timestamp) that has numeric confidence in metadata or output\n",
    "    if conf is None:\n",
    "        for _, md in ranked[1:]:\n",
    "            c_md = md.get(\"confidence\")\n",
    "            if isinstance(c_md, (int, float)):\n",
    "                conf = float(c_md)\n",
    "                break\n",
    "            o = _safe_json_loads(md.get(\"output_json\") or \"\")\n",
    "            if isinstance(o, dict):\n",
    "                c_o = o.get(\"confidence\")\n",
    "                if isinstance(c_o, (int, float)):\n",
    "                    conf = float(c_o)\n",
    "                    break\n",
    "\n",
    "    return {\n",
    "        \"agent_type\": agent_type,\n",
    "        \"timestamp\": best_md.get(\"timestamp\"),\n",
    "        \"risk_level\": str(risk_level).lower(),\n",
    "        \"confidence\": conf,\n",
    "        \"output\": output_obj,\n",
    "        \"_memory_metadata\": best_md,\n",
    "    }\n",
    "\n",
    "def _extract_high_risk_clauses(*, agent_output: Any, agent_type: str, max_items: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract evidence snippets from retrieval matches and label them as high-risk if terms match.\"\"\"\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    if not isinstance(agent_output, dict):\n",
    "        return out\n",
    "    retrieval = agent_output.get(\"retrieval\")\n",
    "    if not isinstance(retrieval, dict):\n",
    "        return out\n",
    "    per_query = retrieval.get(\"per_query\")\n",
    "    if not isinstance(per_query, list):\n",
    "        return out\n",
    "\n",
    "    candidates: List[Tuple[float, Dict[str, Any]]] = []\n",
    "    for item in per_query:\n",
    "        if not isinstance(item, dict):\n",
    "            continue\n",
    "        q = item.get(\"query\")\n",
    "        matches = item.get(\"matches\")\n",
    "        if not isinstance(matches, list):\n",
    "            continue\n",
    "        for m in matches:\n",
    "            if not isinstance(m, dict):\n",
    "                continue\n",
    "            score = m.get(\"score\")\n",
    "            md = m.get(\"metadata\") if isinstance(m.get(\"metadata\"), dict) else {}\n",
    "            snippet = _extract_text_from_match_metadata(md)\n",
    "            snippet_l = snippet.lower()\n",
    "            is_high = any(t in snippet_l for t in HIGH_RISK_TERMS)\n",
    "            score_f = float(score) if isinstance(score, (int, float)) else 0.0\n",
    "            rank_score = score_f + (0.25 if is_high else 0.0)\n",
    "            candidates.append((rank_score, {\n",
    "                \"agent\": agent_type,\n",
    "                \"query\": q,\n",
    "                \"score\": score_f if isinstance(score, (int, float)) else None,\n",
    "                \"snippet\": snippet[:800],\n",
    "                \"is_high_risk\": bool(is_high),\n",
    "            }))\n",
    "\n",
    "    candidates.sort(key=lambda t: t[0], reverse=True)\n",
    "    seen = set()\n",
    "    for _, c in candidates:\n",
    "        key = (c.get(\"agent\"), c.get(\"snippet\"))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        if c.get(\"is_high_risk\"):\n",
    "            out.append(c)\n",
    "        if len(out) >= max_items:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def _overall_risk(agent_risks: Dict[str, str]) -> str:\n",
    "    best = \"low\"\n",
    "    for r in agent_risks.values():\n",
    "        r = (r or \"unknown\").lower()\n",
    "        if RISK_ORDER.get(r, 1) > RISK_ORDER.get(best, 0):\n",
    "            best = r\n",
    "    if best not in {\"low\", \"medium\", \"high\"}:\n",
    "        best = \"medium\"\n",
    "    return best\n",
    "\n",
    "# 1) Retrieve latest outputs\n",
    "latest = {a: get_latest_agent_output(contract_id=CONTRACT_ID, agent_type=a) for a in AGENT_TYPES}\n",
    "\n",
    "# 2) Collect into final JSON\n",
    "final = dict(FINAL_CONTRACT_SCHEMA)\n",
    "final[\"contract_id\"] = CONTRACT_ID\n",
    "final[\"generated_at\"] = _utc_now_iso()\n",
    "\n",
    "agent_risks: Dict[str, str] = {}\n",
    "conf_per_agent: Dict[str, Optional[float]] = {}\n",
    "high_risk_clauses: List[Dict[str, Any]] = []\n",
    "\n",
    "for a in AGENT_TYPES:\n",
    "    payload = latest[a]\n",
    "    output_obj = payload.get(\"output\")\n",
    "    agent_risks[a] = payload.get(\"risk_level\") or _infer_risk_level(output_obj)\n",
    "    conf_per_agent[a] = payload.get(\"confidence\")\n",
    "    # Evidence from structured retrieval (if present)\n",
    "    high_risk_clauses.extend(_extract_high_risk_clauses(agent_output=output_obj, agent_type=a, max_items=5))\n",
    "    # Fallback evidence: scan stored memory text for high-risk terms\n",
    "    if not high_risk_clauses:\n",
    "        mem_text = (payload.get(\"_memory_metadata\") or {}).get(\"output_json\") or \"\"\n",
    "        high_risk_clauses.extend(_extract_term_hits(text=mem_text, agent_type=a, max_items=3))\n",
    "\n",
    "final[\"legal\"] = latest[\"legal_agent\"][\"output\"]\n",
    "final[\"compliance\"] = latest[\"compliance_agent\"][\"output\"]\n",
    "final[\"finance\"] = latest[\"finance_agent\"][\"output\"]\n",
    "final[\"operations\"] = latest[\"operations_agent\"][\"output\"]\n",
    "\n",
    "final[\"overall_risk\"] = _overall_risk(agent_risks)\n",
    "final[\"confidence\"][\"per_agent\"] = conf_per_agent\n",
    "vals = [v for v in conf_per_agent.values() if isinstance(v, (int, float))]\n",
    "final[\"confidence\"][\"overall_avg\"] = (sum(vals) / len(vals)) if vals else None\n",
    "\n",
    "# Keep only high-risk items and cap length\n",
    "final[\"high_risk_clauses\"] = high_risk_clauses[:20]\n",
    "\n",
    "out_path = OUTPUTS_DIR / f\"final_contract_{CONTRACT_ID}.json\"\n",
    "out_path.write_text(json.dumps(final, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved:\", out_path)\n",
    "print(\"overall_risk:\", final[\"overall_risk\"])\n",
    "print(\"confidence overall_avg:\", final[\"confidence\"][\"overall_avg\"])\n",
    "print(\"high_risk_clauses:\", len(final[\"high_risk_clauses\"]))\n",
    "if final[\"confidence\"][\"overall_avg\"] is None:\n",
    "    print(\"NOTE: Confidence is None because older memories may not include a stored confidence score.\")\n",
    "    print(\"      Re-run Section 3 + Section 4 to persist fresh memories; new upserts store confidence in metadata.\")\n",
    "if len(final[\"high_risk_clauses\"]) == 0:\n",
    "    print(\"NOTE: No high-risk evidence snippets were extracted.\")\n",
    "    print(\"      To improve this, ensure your chunk vectors store clause text in metadata (e.g., key 'text' or 'page_content').\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148de70",
   "metadata": {},
   "source": [
    "## 8) Human-Readable Report Template (Executive Summary + Section Bullets)\n",
    "\n",
    "Same data → multiple views: executives don’t read JSON, lawyers want evidence, managers want summaries.\n",
    "\n",
    "This section converts the final JSON into a simple report structure with:\n",
    "- Plain-language **Executive Summary**\n",
    "- Bullet points per section (legal / compliance / finance / operations)\n",
    "- A short list of high-risk clause snippets (evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "017ab57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REPORT PREVIEW\n",
      "================================================================================\n",
      "\n",
      "## Executive Summary\n",
      "\n",
      "Overall risk is high. Confidence score average is 0.558. We found 3 high-risk clause evidence snippets to review.\n",
      "\n",
      "## Overall Risk Assessment\n",
      "\n",
      "- Overall risk level: high\n",
      "- Confidence (avg): 0.5580853783255555\n",
      "- Legal confidence: 0.6606792805499999\n",
      "- Compliance confidence: None\n",
      "- Finance confidence: 0.54319379216\n",
      "- Operations confidence: 0.47038306226666665\n",
      "\n",
      "## Legal Analysis\n",
      "\n",
      "- Key legal obligations summarized from retrieval outputs.\n",
      "- Review termination, breach, and indemnity language if present.\n",
      "\n",
      "## Compliance Analysis\n",
      "\n",
      "- Key privacy/security/compliance obligations summarized from retrieval outputs.\n",
      "- Review audit rights, incident notification, and data handling language if present.\n",
      "\n",
      "## Financial Analysis\n",
      "\n",
      "- Key payment, invoicing, and late-fee obligations summarized from retrieval outputs.\n",
      "- Review liability and penalty exposure if present.\n",
      "\n",
      "## Operational Analysis\n",
      "\n",
      "- Key deliverables, timelines, and SLA obligations summarized from retrieval outputs.\n",
      "- Review uptime commitments and service credits if present.\n",
      "\n",
      "## Conclusion & Recommendations\n",
      "\n",
      "- Prioritize review of the high-risk clauses listed below.\n",
      "- If overall risk is high, consider negotiation points or approvals before signing.\n",
      "- High-risk evidence:\n",
      "- [legal_agent] (term: breach) se in law or in equity or otherwise, upon the occurrence of any one or more of the following events: i. The other party breaches or fails to perform any of its material obligations provided f\n",
      "- [legal_agent] (term: indemn) ls); Section 7 (Dispute Resolution); Section 8 (Confidentiality); Section 9 (Intellectual Property Rights); Section 10 (Indemnities), Section 11 (Liability) and Section 12 (Miscellaneous Prov\n"
     ]
    }
   ],
   "source": [
    "REPORT_STRUCTURE = [\n",
    "    \"Executive Summary\",\n",
    "    \"Overall Risk Assessment\",\n",
    "    \"Legal Analysis\",\n",
    "    \"Compliance Analysis\",\n",
    "    \"Financial Analysis\",\n",
    "    \"Operational Analysis\",\n",
    "    \"Conclusion & Recommendations\",\n",
    "]\n",
    "\n",
    "def _bulletize(lines: List[str]) -> str:\n",
    "    return \"\\n\".join([f\"- {ln}\" for ln in lines if isinstance(ln, str) and ln.strip()])\n",
    "\n",
    "def _clean_snippet(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    # Normalize common escaped forms from JSON previews\n",
    "    s = s.replace(\"\\\\n\", \" \").replace(\"\\n\", \" \")\n",
    "    s = s.replace(\"\\\\\\\"\", '\"').replace(\"\\\\/\", \"/\")\n",
    "    # Collapse whitespace\n",
    "    s = \" \".join(s.split())\n",
    "    return s.strip()\n",
    "\n",
    "def _looks_like_json_fragment(s: str) -> bool:\n",
    "    \"\"\"Detect snippets that are likely cut-through JSON rather than readable clause text.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return True\n",
    "    t = s.strip()\n",
    "    if not t:\n",
    "        return True\n",
    "    tl = t.lower()\n",
    "\n",
    "    # Hard-block: retrieval/memory JSON keys (these are NOT clause text)\n",
    "    hard_markers = [\n",
    "        \"top_k_per_query\",\n",
    "        \"filter_chunks_by_contract_id\",\n",
    "        \"per_query\",\n",
    "        \"matches\",\n",
    "        \"retrieval\\\":\",\n",
    "        \"metadata\\\":\",\n",
    "        \"output_json\",\n",
    "        \"raw_output_json\",\n",
    "    ]\n",
    "    if any(m in tl for m in hard_markers):\n",
    "        return True\n",
    "\n",
    "    # Soft heuristics: many JSON-ish tokens and low natural-language signal\n",
    "    jsonish = sum(t.count(ch) for ch in [\"{\", \"}\", \"[\", \"]\", \":\"])\n",
    "    backslashes = t.count(\"\\\\\")\n",
    "    quotes = t.count('\"')\n",
    "    letters = sum(ch.isalpha() for ch in t)\n",
    "    spaces = t.count(\" \")\n",
    "\n",
    "    if (jsonish + backslashes + quotes) >= 8 and (letters < 60 or spaces < 8):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def build_executive_summary(final_json: Dict[str, Any]) -> str:\n",
    "    risk = (final_json.get(\"overall_risk\") or \"medium\").lower()\n",
    "    conf = final_json.get(\"confidence\", {}).get(\"overall_avg\")\n",
    "    conf_s = \"unknown\" if conf is None else f\"{conf:.3f}\"\n",
    "    n_hi = len(final_json.get(\"high_risk_clauses\") or [])\n",
    "    # Simple language, no legal jargon\n",
    "    return (\n",
    "        f\"Overall risk is {risk}. \"\n",
    "        f\"Confidence score average is {conf_s}. \"\n",
    "        f\"We found {n_hi} high-risk clause evidence snippets to review.\"\n",
    "    )\n",
    "\n",
    "def build_report(final_json: Dict[str, Any]) -> Dict[str, str]:\n",
    "    per_agent_conf = (final_json.get(\"confidence\") or {}).get(\"per_agent\") or {}\n",
    "    hi = final_json.get(\"high_risk_clauses\") or []\n",
    "\n",
    "    report: Dict[str, str] = {}\n",
    "    report[\"Executive Summary\"] = build_executive_summary(final_json)\n",
    "\n",
    "    report[\"Overall Risk Assessment\"] = _bulletize([\n",
    "        f\"Overall risk level: {(final_json.get('overall_risk') or 'medium').lower()}\",\n",
    "        f\"Confidence (avg): {final_json.get('confidence', {}).get('overall_avg')}\",\n",
    "        f\"Legal confidence: {per_agent_conf.get('legal_agent')}\",\n",
    "        f\"Compliance confidence: {per_agent_conf.get('compliance_agent')}\",\n",
    "        f\"Finance confidence: {per_agent_conf.get('finance_agent')}\",\n",
    "        f\"Operations confidence: {per_agent_conf.get('operations_agent')}\",\n",
    "    ])\n",
    "\n",
    "    # Per-section bullets (keep it generic; the evidence list carries the detail)\n",
    "    report[\"Legal Analysis\"] = _bulletize([\n",
    "        \"Key legal obligations summarized from retrieval outputs.\",\n",
    "        \"Review termination, breach, and indemnity language if present.\",\n",
    "    ])\n",
    "    report[\"Compliance Analysis\"] = _bulletize([\n",
    "        \"Key privacy/security/compliance obligations summarized from retrieval outputs.\",\n",
    "        \"Review audit rights, incident notification, and data handling language if present.\",\n",
    "    ])\n",
    "    report[\"Financial Analysis\"] = _bulletize([\n",
    "        \"Key payment, invoicing, and late-fee obligations summarized from retrieval outputs.\",\n",
    "        \"Review liability and penalty exposure if present.\",\n",
    "    ])\n",
    "    report[\"Operational Analysis\"] = _bulletize([\n",
    "        \"Key deliverables, timelines, and SLA obligations summarized from retrieval outputs.\",\n",
    "        \"Review uptime commitments and service credits if present.\",\n",
    "    ])\n",
    "\n",
    "    # Evidence: prefer readable snippets (skip JSON fragments from truncated memory previews)\n",
    "    top_evidence: List[str] = []\n",
    "    for item in hi:\n",
    "        if not isinstance(item, dict):\n",
    "            continue\n",
    "        raw = item.get(\"snippet\") or \"\"\n",
    "        snippet = _clean_snippet(raw)\n",
    "        if not snippet:\n",
    "            continue\n",
    "        if _looks_like_json_fragment(snippet):\n",
    "            continue\n",
    "        agent = item.get(\"agent\") or \"unknown_agent\"\n",
    "        term = item.get(\"matched_term\")\n",
    "        prefix = f\"[{agent}] \" + (f\"(term: {term}) \" if isinstance(term, str) and term else \"\")\n",
    "        top_evidence.append((prefix + snippet)[:220])\n",
    "        if len(top_evidence) >= 8:\n",
    "            break\n",
    "\n",
    "    report[\"Conclusion & Recommendations\"] = _bulletize([\n",
    "        \"Prioritize review of the high-risk clauses listed below.\",\n",
    "        \"If overall risk is high, consider negotiation points or approvals before signing.\",\n",
    "        *([\"High-risk evidence:\"] + top_evidence if top_evidence else [\"No clean high-risk evidence snippets were extracted from memory.\"]),\n",
    "    ])\n",
    "\n",
    "    return report\n",
    "\n",
    "report = build_report(final)\n",
    "\n",
    "print(\"\\n\".join([\"=\" * 80, \"REPORT PREVIEW\", \"=\" * 80]))\n",
    "for section in REPORT_STRUCTURE:\n",
    "    print(f\"\\n## {section}\\n\")\n",
    "    print(report.get(section, \"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
