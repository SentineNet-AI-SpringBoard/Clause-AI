{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Milestone 3\n"
   ],
   "metadata": {
    "id": "vlmPOWGiO02v"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "id": "GPCuJ0adWK8w"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "ARTIFACTS_DIR = ROOT / \"artifacts\"\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# Avoid optional TensorFlow/JAX imports (common Windows DLL issues, and not needed here)\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_TF\", \"1\")\n",
    "os.environ.setdefault(\"USE_TF\", \"0\")\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_FLAX\", \"1\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(\"milestone3\")\n",
    "\n",
    "def utc_now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def load_env_file(path: Path) -> Dict[str, str]:\n",
    "    \"\"\"Minimal .env loader (no extra deps).\n",
    "\n",
    "    - Skips blanks and comments\n",
    "    - Supports KEY=VALUE\n",
    "    - Strips surrounding quotes\n",
    "    - Does NOT override already-set env vars\n",
    "    \"\"\"\n",
    "    loaded: Dict[str, str] = {}\n",
    "    if not path.exists():\n",
    "        return loaded\n",
    "\n",
    "    for raw_line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        line = raw_line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        if \"=\" not in line:\n",
    "            continue\n",
    "        key, value = line.split(\"=\", 1)\n",
    "        key = key.strip()\n",
    "        value = value.strip().strip('\"').strip(\"'\")\n",
    "        if not key:\n",
    "            continue\n",
    "        if os.getenv(key) is None:\n",
    "            os.environ[key] = value\n",
    "            loaded[key] = value\n",
    "    return loaded\n",
    "\n",
    "# Auto-load root .env if present\n",
    "env_loaded = load_env_file(ROOT / \".env\")\n",
    "\n",
    "logger.info(f\"ROOT={ROOT}\")\n",
    "logger.info(f\"ARTIFACTS_DIR={ARTIFACTS_DIR} (exists={ARTIFACTS_DIR.exists()})\")\n",
    "logger.info(f\"Loaded .env keys: {sorted(env_loaded.keys())}\")\n",
    "logger.info(\"Env guards: TRANSFORMERS_NO_TF=%s USE_TF=%s TRANSFORMERS_NO_FLAX=%s\", os.getenv(\"TRANSFORMERS_NO_TF\"), os.getenv(\"USE_TF\"), os.getenv(\"TRANSFORMERS_NO_FLAX\"))"
   ],
   "metadata": {
    "id": "HOzr4shCeZ5J"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parallel Agent Execution with LangGraph"
   ],
   "metadata": {
    "id": "30tqEeFAeqAW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Shared agent list used across the notebook\n",
    "AGENT_TYPES = [\"legal_agent\", \"compliance_agent\", \"finance_agent\", \"operations_agent\"]"
   ],
   "metadata": {
    "id": "feX4bu8_eqMD"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Dependency repair for this notebook kernel (run once if imports fail)\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import importlib.metadata as _md  # py3.8+\n",
    "except Exception:  # pragma: no cover\n",
    "    _md = None\n",
    "\n",
    "def _v(pkg: str) -> str:\n",
    "    if _md is None:\n",
    "        return \"(unknown)\"\n",
    "    try:\n",
    "        return _md.version(pkg)\n",
    "    except Exception:\n",
    "        return \"(not installed)\"\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Before:\")\n",
    "print(\"- huggingface-hub:\", _v(\"huggingface-hub\"))\n",
    "print(\"- transformers:   \", _v(\"transformers\"))\n",
    "print(\"- sentence-transformers:\", _v(\"sentence-transformers\"))\n",
    "print(\"- tensorflow:     \", _v(\"tensorflow\"))\n",
    "print(\"- tensorflow-intel:\", _v(\"tensorflow-intel\"))\n",
    "\n",
    "# IMPORTANT: %pip installs into the currently-running Jupyter kernel environment.\n",
    "# If you still see the same import error after this, restart the kernel and rerun from Cell 3.\n",
    "%pip install -U \"huggingface-hub>=0.24.0,<1.0\" \"transformers>=4.40.0\" \"sentence-transformers>=2.7.0\"\n",
    "\n",
    "# If you see: \"Failed to load the native TensorFlow runtime\" on Windows, TensorFlow is installed but broken.\n",
    "# Sentence-transformers does not require TensorFlow for embeddings, so removing it is safe for this notebook:\n",
    "# %pip uninstall -y tensorflow tensorflow-intel\n",
    "\n",
    "print(\"\\nAfter:\")\n",
    "print(\"- huggingface-hub:\", _v(\"huggingface-hub\"))\n",
    "print(\"- transformers:   \", _v(\"transformers\"))\n",
    "print(\"- sentence-transformers:\", _v(\"sentence-transformers\"))\n",
    "print(\"- tensorflow:     \", _v(\"tensorflow\"))\n",
    "print(\"- tensorflow-intel:\", _v(\"tensorflow-intel\"))\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vt3QBaU2eqob",
    "outputId": "6b4e720b-efce-4643-941b-524b8ab71637"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Python executable: /usr/bin/python3\n",
      "Before:\n",
      "- huggingface-hub: 0.36.0\n",
      "- transformers:    4.57.6\n",
      "- sentence-transformers: 5.2.0\n",
      "- tensorflow:      2.19.0\n",
      "- tensorflow-intel: (not installed)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
      "Requirement already satisfied: transformers>=4.40.0 in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
      "Requirement already satisfied: sentence-transformers>=2.7.0 in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0) (3.20.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (0.7.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.7.0) (2.9.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.7.0) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.7.0) (1.16.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.7.0) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.7.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.7.0) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.7.0) (3.1.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.24.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.24.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.24.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.24.0) (2026.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers>=2.7.0) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers>=2.7.0) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.7.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.7.0) (3.0.3)\n",
      "\n",
      "After:\n",
      "- huggingface-hub: 0.36.0\n",
      "- transformers:    4.57.6\n",
      "- sentence-transformers: 5.2.0\n",
      "- tensorflow:      2.19.0\n",
      "- tensorflow-intel: (not installed)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    " # Dependency repair for this notebook kernel (run once if imports fail)\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import importlib.metadata as _md  # py3.8+\n",
    "except Exception:  # pragma: no cover\n",
    "    _md = None\n",
    "\n",
    "def _v(pkg: str) -> str:\n",
    "    if _md is None:\n",
    "        return \"(unknown)\"\n",
    "    try:\n",
    "        return _md.version(pkg)\n",
    "    except Exception:\n",
    "        return \"(not installed)\"\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Before:\")\n",
    "print(\"- huggingface-hub:\", _v(\"huggingface-hub\"))\n",
    "print(\"- transformers:   \", _v(\"transformers\"))\n",
    "print(\"- sentence-transformers:\", _v(\"sentence-transformers\"))\n",
    "print(\"- tensorflow:     \", _v(\"tensorflow\"))\n",
    "print(\"- tensorflow-intel:\", _v(\"tensorflow-intel\"))\n",
    "\n",
    "# IMPORTANT: %pip installs into the currently-running Jupyter kernel environment.\n",
    "# If you still see the same import error after this, restart the kernel and rerun from Cell 3.\n",
    "%pip install -U \"huggingface-hub>=0.24.0,<1.0\" \"transformers>=4.40.0\" \"sentence-transformers>=2.7.0\"\n",
    "\n",
    "# If you see: \"Failed to load the native TensorFlow runtime\" on Windows, TensorFlow is installed but broken.\n",
    "# Sentence-transformers does not require TensorFlow for embeddings, so removing it is safe for this notebook:\n",
    "# %pip uninstall -y tensorflow tensorflow-intel\n",
    "\n",
    "print(\"\\nAfter:\")\n",
    "print(\"- huggingface-hub:\", _v(\"huggingface-hub\"))\n",
    "print(\"- transformers:   \", _v(\"transformers\"))\n",
    "print(\"- sentence-transformers:\", _v(\"sentence-transformers\"))\n",
    "print(\"- tensorflow:     \", _v(\"tensorflow\"))\n",
    "print(\"- tensorflow-intel:\", _v(\"tensorflow-intel\"))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ePjw4-DSetFm",
    "outputId": "e00194ee-3f4f-4d14-e0d1-1044e18db6de"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Python executable: /usr/bin/python3\n",
      "Before:\n",
      "- huggingface-hub: 0.36.0\n",
      "- transformers:    4.57.6\n",
      "- sentence-transformers: 5.2.0\n",
      "- tensorflow:      2.19.0\n",
      "- tensorflow-intel: (not installed)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
      "Requirement already satisfied: transformers>=4.40.0 in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
      "Requirement already satisfied: sentence-transformers>=2.7.0 in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0) (3.20.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (0.7.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.7.0) (2.9.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.7.0) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.7.0) (1.16.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.7.0) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.7.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.7.0) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.7.0) (3.1.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.24.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.24.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.24.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.24.0) (2026.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers>=2.7.0) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers>=2.7.0) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.7.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.7.0) (3.0.3)\n",
      "\n",
      "After:\n",
      "- huggingface-hub: 0.36.0\n",
      "- transformers:    4.57.6\n",
      "- sentence-transformers: 5.2.0\n",
      "- tensorflow:      2.19.0\n",
      "- tensorflow-intel: (not installed)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "gnd5Pfzsm1uO"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vector Database Connection and Embedding Setup"
   ],
   "metadata": {
    "id": "HMIj8IYtnA6r"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U pinecone-client sentence-transformers transformers huggingface-hub\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "NtInEAOEtryI",
    "outputId": "c0754807-8f9f-48a4-d4d8-9d20ffc6b2b3"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pinecone-client\n",
      "  Using cached pinecone_client-6.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
      "Collecting huggingface-hub\n",
      "  Using cached huggingface_hub-1.3.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone-client) (2026.1.4)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone-client) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone-client) (4.15.0)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone-client) (2.5.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Using cached pinecone_client-6.0.0-py3-none-any.whl (6.7 kB)\n",
      "Installing collected packages: pinecone-client\n",
      "Successfully installed pinecone-client-6.0.0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "pinecone"
        ]
       },
       "id": "836f75c9ce994ba58d1dbd7a778b54d9"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip uninstall -y pinecone-client\n",
    "!pip uninstall -y pinecone\n",
    "!pip uninstall -y pinecone-core\n",
    "!pip uninstall -y pinecone-text\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRwDJUxqwP5W",
    "outputId": "23483203-0528-4d28-bf94-7f25b0e89557"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found existing installation: pinecone-client 6.0.0\n",
      "Uninstalling pinecone-client-6.0.0:\n",
      "  Successfully uninstalled pinecone-client-6.0.0\n",
      "Found existing installation: pinecone 8.0.0\n",
      "Uninstalling pinecone-8.0.0:\n",
      "  Successfully uninstalled pinecone-8.0.0\n",
      "\u001b[33mWARNING: Skipping pinecone-core as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping pinecone-text as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U pinecone\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10q2toV7wYsT",
    "outputId": "328506be-1a05-486e-8c08-35131f3d7e54"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pinecone\n",
      "  Using cached pinecone-8.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2026.1.4)\n",
      "Requirement already satisfied: orjson>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from pinecone) (3.11.5)\n",
      "Requirement already satisfied: pinecone-plugin-assistant<4.0.0,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from pinecone) (3.0.1)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.1.0,>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from pinecone) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone) (4.15.0)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.5.0)\n",
      "Requirement already satisfied: packaging<25.0,>=24.2 in /usr/local/lib/python3.12/dist-packages (from pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone) (24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone) (2.32.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone) (3.11)\n",
      "Using cached pinecone-8.0.0-py3-none-any.whl (745 kB)\n",
      "Installing collected packages: pinecone\n",
      "Successfully installed pinecone-8.0.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pinecone\n",
    "print(\"pinecone imported OK\")\n",
    "print(pinecone.__file__)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZLeQSENcwcYo",
    "outputId": "de46f26b-37b3-45f6-8bc7-30dcb3a2c22d"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pinecone imported OK\n",
      "/usr/local/lib/python3.12/dist-packages/pinecone/__init__.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Pinecone connection\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# Ensure optional TensorFlow\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_TF\", \"1\")\n",
    "os.environ.setdefault(\"USE_TF\", \"0\")\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_FLAX\", \"1\")\n",
    "\n",
    "# =============================================================\n",
    "# \ud83d\udd11 HARD-CODED CONFIG (AS REQUESTED)\n",
    "# =============================================================\n",
    "\n",
    "PINECONE_API_KEY = \"pcsk_6jbLBU_DxNgioCN5BHBNM6x3S2Gd9WMY3DDVnruCFBSsEa7efABnmRWydJhEn4itJDVfG\"\n",
    "INDEX_NAME = \"pinevs\"\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Optional (only needed for OLD Pinecone SDK)\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "\n",
    "# =============================================================\n",
    "# CONNECT TO PINECONE\n",
    "# =============================================================\n",
    "\n",
    "index = None\n",
    "try:\n",
    "    # New SDK\n",
    "    from pinecone import Pinecone\n",
    "\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    index = pc.Index(INDEX_NAME)\n",
    "    logger.info(f\"\u2705 Connected to Pinecone index '{INDEX_NAME}' (new SDK)\")\n",
    "\n",
    "except Exception as e_new:\n",
    "    try:\n",
    "        # Old SDK fallback\n",
    "        import pinecone\n",
    "\n",
    "        if not PINECONE_ENV:\n",
    "            raise RuntimeError(\n",
    "                \"Legacy Pinecone SDK requires PINECONE_ENV \"\n",
    "                \"(e.g., 'us-east1-gcp')\"\n",
    "            )\n",
    "\n",
    "        pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "        index = pinecone.Index(INDEX_NAME)\n",
    "        logger.info(f\"\u2705 Connected to Pinecone index '{INDEX_NAME}' (legacy SDK)\")\n",
    "\n",
    "    except Exception as e_old:\n",
    "        raise RuntimeError(f\"\u274c Pinecone connection failed: new={e_new} old={e_old}\")\n",
    "\n",
    "# =============================================================\n",
    "# EMBEDDING MODEL\n",
    "# =============================================================\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "CACHE_DIR = ROOT / \"models_cache\" / \"hub\"\n",
    "\n",
    "logger.info(f\"\ud83d\udd39 Loading embedding model: {MODEL_NAME}\")\n",
    "model = SentenceTransformer(MODEL_NAME, cache_folder=str(CACHE_DIR))\n",
    "\n",
    "def embed_query(text: str) -> List[float]:\n",
    "    return model.encode(\n",
    "        [text],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )[0].tolist()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511,
     "referenced_widgets": [
      "e4ce0f39c3da4495b0d5081f0ba07c8c",
      "9c0d1b3efe524688acc11f7d30a6375d",
      "be39b461931749f8ad1cca9ea033d2ae",
      "548e1bdca7f641908b44eb4458ceac2f",
      "b000816729a04c79832640a68c3f3d05",
      "67acb541b69e47fa8af98c5c187b9792",
      "c139305c8f4c4ffe85c0bd516c47232b",
      "30e28fd1629d4e56b74fc8478c65eb4e",
      "988d4a8f85cf480ebfe3ac026c250fb9",
      "36884a3d1e51454397504d0079f99067",
      "0acae79fe05043b9bebea5da3b7f1396",
      "84fb65a318074720a99e674cf6bca394",
      "a93bea5af93c43f79f1d8eb4ca29a835",
      "7245a10bbfcb4553a6d0cce71b6de541",
      "c84ef642d7814eec9ccea90291090ba5",
      "952d780d5c9b4f909710921fd370d263",
      "99d3ccb8dd244f3e902aced97741551b",
      "ced72aed6f444506934b9e0c5bccc813",
      "e80b18d683e84fd2af7b3350a0011a8f",
      "a9106b98c4014ccfb2d1549143cb5a8b",
      "527502febc2d4794bfe6433af2855fb0",
      "c64ba474129d4e10acb5c2252fcc3f53",
      "3653b0e47453425d89ce43b8adc39622",
      "85a59117e6ac438195cdfdd163e3d25d",
      "83ad4eed446240b2a204405693a6926e",
      "08027dc65f1240f7a3e75497205ec5a2",
      "d52af2bcf67c4f24bf4e135cb9e3dacf",
      "bfcb2a43022e4d12a6997058b0090db0",
      "48df14775b9a4ed69f1b721a9cd7c675",
      "4f7e32d825ed4b49902488675b32972d",
      "281415e65b124ae8af7fabb82e002f7f",
      "51de368d91bf4952942b37a02fb03c1b",
      "19e8101ce42748ab9cfdcc1e3aecab7c",
      "b516ba445b3a492a83753dcdf507ac71",
      "1b723682f80c43cdb0c6b933efbb2539",
      "fb035c8623514e32a3c181e01a6e3704",
      "a28738d5073648baa15994b998a10453",
      "cee8feceb8f444289ee89e61873df20c",
      "27e1520768274a75bf715bbedc0fb524",
      "64ec00b9d2ac406499af8b632bf674d8",
      "5717c433d66a47a99340548b449b5dca",
      "bf1f58f5bf6948f8b606345290f7c19c",
      "5c02ac7cceac48978a8c8e76230ea216",
      "caf02b5eace64af8af361dfa6fa9e521",
      "86400d938b104d909450b13334ccc77b",
      "5ff117ac10534f3e9c77a7aeb10d359b",
      "1f4389d4cd4c4957a4f66cff3442ea38",
      "33f4d7177d8c46b18c306e5776b79efd",
      "19b1f673c9ae44589efa4125e2624189",
      "a9c2633e068446b2b54ae3c0069207aa",
      "8aa77931f12f4ac8a7c846edc7c13068",
      "691068f9a19248cea2f3c3d67d1d8dc0",
      "d2543a9d989d4f81957e9df26b55c7dd",
      "f923838f054d4d15ace6054e962e9651",
      "98158610e8494481858bf2805e007eea",
      "9b77b44752534082a8da442ffed9b73a",
      "a4d0719ab00843ada11fbfe1fc299e84",
      "1e09166392a348bfafe6ab7db07f965a",
      "299da1aab47c49b3a7d2721195b80c06",
      "c21dda0059d2439eb38a4a80a67be0ef",
      "60a1ee70712d401f85d77dd797ed5312",
      "58de1abf82d448b59f784f5966d4e8fc",
      "ee3a9f08735c4606b4c9d03a53529e5a",
      "fe3440c7abca4148bcf3e40e29743c1a",
      "ca4ec9705fa248a49acd15e353c7f32d",
      "229fdfd71588490e815add312c739849",
      "f7a16bdc1f624f919934e4485b30d1a3",
      "729e83c4f7be47c1848a855b8b9fc0f4",
      "2455121c67bf422cab7365a41b05cf9f",
      "1f11c79af5aa4b679a598a1aaf975fb0",
      "96d5b3aa9f8b467591abfb415a734cf1",
      "2475ecfd291648e2b96aae38feb47433",
      "9d7bb1d0c87c427f9d2a576f52974a21",
      "c75210e7daa248c29d866a22a247f3d2",
      "ab816364ec6c479eb59b5a7856a218bc",
      "e04b1babfe1b46d6b63a59ae639002a6",
      "fe7ea2870ba74b40a16bf71e2b6d9634",
      "ff91abba46534a42b4ef7182c3f350ce",
      "7ca1d5bf605b4cb39573085c77b257c8",
      "b969903afb8f429a82030d3e1c60cd30",
      "6c5ed4ce1e5544cfbd09d6a3984ee00b",
      "2b684b8bf02d4a97a31b9877ae09de9e",
      "414132c0b6044132a259f2d343603a0c",
      "845ee630c7ff4e7ea91fa9a8b0dd377c",
      "950e817858d04ce0bb315922f35b2f1d",
      "4aab2052b17740c398b97ea4a546b2a1",
      "1d0b400582414745993dd52076682d1c",
      "5b82b3c42470430f9ef76a37b6f7c808",
      "4727633f15054f02a5cff8382cc4f400",
      "033c4c7db2a84cf49bbf56d556794216",
      "ef3a3ebc87314a3ea9585fda07d69ee7",
      "f4afb2376fe04282805fb42d187d17a6",
      "e9b8c72c29c14821bd73193de6a42444",
      "f40278ce1d4f44b5a76fd00a989ce844",
      "486d8d058bfe43818c2bb3e340737366",
      "acf68bcb58564c43a7ad5a8d477a423b",
      "ed0a4321e1cb4cf9bb972505bf27ea61",
      "70a6de6774844c9c9eeabd69849066ef",
      "9ccd840f3a804f1d8fdf588072d41162",
      "692afe8e5ad842a8806cbe7cbf7a0a89",
      "d925ae83f9a644c283694c39e4571d65",
      "52dfe4897ba5400a958071a15227da1e",
      "357d19089d934d6eabff883093b652e2",
      "405ce1d16dda46b6824a2477b3114aff",
      "7d78f591388f472aaa0886a1857c9f67",
      "0cc0944b456f4a73b917f9da97039871",
      "8977afbbf2974065b9f6d1febd7d574d",
      "164306742ae442c7850e15e09d9e67d4",
      "1e85cc09aa6a4e01a6a252482601b415",
      "4c1015d186ee4b08bc087551bf24ae72",
      "56267188286e401ab9b509c8e2296b7d",
      "a1e7466f398c46e7b2cac85d70d483a6",
      "692f67644da04028856412a2e99b3465",
      "a16482ae0dc048ce9bf131c5289b2c66",
      "0c4e94dceb76465a80a381ca52432d8d",
      "4c6f52488e8e4f29bc27fe7e26916ed5",
      "6a45245980c444e184977b3ae6ca5d72",
      "7e3c7da623a844c390a435a7719f9ff3",
      "f8f3800b5aa6416e93927d73436084e3",
      "daea1dbee3f145c6b3572562f8a844a7",
      "369534c985354ec18b84de4541576597"
     ]
    },
    "id": "dFCWQsi-nBck",
    "outputId": "2db33bda-6aaf-42f3-f311-e7a684bccf30"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4ce0f39c3da4495b0d5081f0ba07c8c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84fb65a318074720a99e674cf6bca394"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3653b0e47453425d89ce43b8adc39622"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b516ba445b3a492a83753dcdf507ac71"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86400d938b104d909450b13334ccc77b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b77b44752534082a8da442ffed9b73a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7a16bdc1f624f919934e4485b30d1a3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff91abba46534a42b4ef7182c3f350ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4727633f15054f02a5cff8382cc4f400"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "692afe8e5ad842a8806cbe7cbf7a0a89"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56267188286e401ab9b509c8e2296b7d"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Context-First Agent Orchestration\n",
    "**bold text**\n",
    "Agents fetch relevant knowledge upfront, then execute tasks using either sequential or parallel execution strategies."
   ],
   "metadata": {
    "id": "30JR4Wkz1eU_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "import time\n",
    "import asyncio\n"
   ],
   "metadata": {
    "id": "8wgGCsor2HP2"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "AGENT_TYPES = [\"legal_agent\", \"compliance_agent\", \"finance_agent\", \"operations_agent\"]\n",
    "\n",
    "AGENT_QUERIES: Dict[str, List[str]] = {\n",
    "\n",
    "    \"legal_agent\": [\n",
    "        \"Can you explain the termination clauses in this contract?\",\n",
    "        \"What are the consequences if someone breaches the contract?\",\n",
    "        \"Which obligations relate to confidentiality and non-disclosure?\",\n",
    "        \"What indemnity and protection responsibilities exist for parties?\",\n",
    "    ],\n",
    "\n",
    "    \"compliance_agent\": [\n",
    "        \"What privacy and data protection rules apply?\",\n",
    "        \"Which regulations must we comply with?\",\n",
    "        \"What auditing and reporting steps are required?\",\n",
    "        \"How long must data be kept, and when should it be deleted?\",\n",
    "        \"What steps must be taken to report a security breach?\",\n",
    "        \"Are there specific certifications or audits needed (SOC2/ISO/HIPAA)?\",\n",
    "    ],\n",
    "\n",
    "    \"finance_agent\": [\n",
    "        \"What are the payment deadlines and conditions?\",\n",
    "        \"How are fees, invoices, and billing handled?\",\n",
    "        \"What penalties or late fees are applied for missed payments?\",\n",
    "        \"What interest is charged on overdue payments?\",\n",
    "        \"What financial liabilities does each party carry?\",\n",
    "    ],\n",
    "\n",
    "    \"operations_agent\": [\n",
    "        \"What are the expected deliverables and outputs of this project?\",\n",
    "        \"What timelines and milestones should be met?\",\n",
    "        \"What service level agreements (SLAs) are defined?\",\n",
    "        \"What standards of performance must be maintained?\",\n",
    "        \"What operational responsibilities are assigned to each party?\",\n",
    "        \"What guarantees exist for uptime and service continuity?\",\n",
    "    ],\n",
    "\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# The rest of your pipeline code remains exactly the same\n",
    "# -------------------------\n",
    "def pinecone_query(\n",
    "    *,\n",
    "    query: str,\n",
    "    top_k: int = 5,\n",
    "    namespace: Optional[str] = None,\n",
    "    metadata_filter: Optional[Dict[str, Any]] = None,\n",
    ") -> Any:\n",
    "    qvec = embed_query(query)\n",
    "    kwargs: Dict[str, Any] = {\n",
    "        \"vector\": qvec,\n",
    "        \"top_k\": top_k,\n",
    "        \"include_metadata\": True,\n",
    "    }\n",
    "    if namespace is not None:\n",
    "        kwargs[\"namespace\"] = namespace\n",
    "    if metadata_filter is not None:\n",
    "        kwargs[\"filter\"] = metadata_filter\n",
    "    return index.query(**kwargs)\n",
    "\n",
    "def _extract_matches(resp: Any) -> List[Dict[str, Any]]:\n",
    "    matches = getattr(resp, \"matches\", None)\n",
    "    if matches is None and isinstance(resp, dict):\n",
    "        matches = resp.get(\"matches\")\n",
    "    if not matches:\n",
    "        return []\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for m in matches:\n",
    "        md = getattr(m, \"metadata\", None)\n",
    "        score = getattr(m, \"score\", None)\n",
    "        if md is None and isinstance(m, dict):\n",
    "            md = m.get(\"metadata\")\n",
    "            score = m.get(\"score\")\n",
    "        out.append({\n",
    "            \"score\": float(score) if score is not None else None,\n",
    "            \"metadata\": md or {},\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def _confidence_from_matches(matches: List[Dict[str, Any]]) -> Optional[float]:\n",
    "    scores = [m.get(\"score\") for m in matches if isinstance(m.get(\"score\"), (int, float))]\n",
    "    if not scores:\n",
    "        return None\n",
    "    return float(sum(scores) / len(scores))\n",
    "\n",
    "def run_agent_pipeline(\n",
    "    *,\n",
    "    agent_type: str,\n",
    "    question: str,\n",
    "    contract_id: str,\n",
    "    top_k_per_query: int = 5,\n",
    "    chunks_namespace: Optional[str] = None,\n",
    "    filter_chunks_by_contract_id: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    if agent_type not in AGENT_QUERIES:\n",
    "        raise ValueError(f\"Unknown agent_type: {agent_type}\")\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    md_filter = (\n",
    "        {\"contract_id\": {\"$eq\": contract_id}}\n",
    "        if filter_chunks_by_contract_id\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    all_matches: List[Dict[str, Any]] = []\n",
    "    per_query: List[Dict[str, Any]] = []\n",
    "    for q in AGENT_QUERIES[agent_type]:\n",
    "        resp = pinecone_query(query=q, top_k=top_k_per_query, namespace=chunks_namespace, metadata_filter=md_filter)\n",
    "        matches = _extract_matches(resp)\n",
    "        per_query.append({\"query\": q, \"matches\": matches})\n",
    "        all_matches.extend(matches)\n",
    "\n",
    "    confidence = _confidence_from_matches(all_matches)\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    return {\n",
    "        \"agent_type\": agent_type,\n",
    "        \"contract_id\": contract_id,\n",
    "        \"question\": question,\n",
    "        \"timestamp\": utc_now_iso(),\n",
    "        \"elapsed_seconds\": elapsed,\n",
    "        \"confidence\": confidence,\n",
    "        \"retrieval\": {\n",
    "            \"top_k_per_query\": top_k_per_query,\n",
    "            \"filter_chunks_by_contract_id\": filter_chunks_by_contract_id,\n",
    "            \"per_query\": per_query,\n",
    "        },\n",
    "    }\n",
    "\n",
    "def run_sequential(\n",
    "    *,\n",
    "    question: str,\n",
    "    contract_id: str,\n",
    "    agents: List[str] = AGENT_TYPES,\n",
    "    filter_chunks_by_contract_id: bool = False,\n",
    ") -> Tuple[Dict[str, Any], float]:\n",
    "    t0 = time.perf_counter()\n",
    "    out: Dict[str, Any] = {}\n",
    "    for a in agents:\n",
    "        out[a] = run_agent_pipeline(\n",
    "            agent_type=a,\n",
    "            question=question,\n",
    "            contract_id=contract_id,\n",
    "            filter_chunks_by_contract_id=filter_chunks_by_contract_id,\n",
    "        )\n",
    "    return out, time.perf_counter() - t0\n",
    "\n",
    "async def run_parallel(\n",
    "    *,\n",
    "    question: str,\n",
    "    contract_id: str,\n",
    "    agents: List[str] = AGENT_TYPES,\n",
    "    filter_chunks_by_contract_id: bool = False,\n",
    ") -> Tuple[Dict[str, Any], float]:\n",
    "    t0 = time.perf_counter()\n",
    "    tasks = [\n",
    "        asyncio.to_thread(\n",
    "            run_agent_pipeline,\n",
    "            agent_type=a,\n",
    "            question=question,\n",
    "            contract_id=contract_id,\n",
    "            filter_chunks_by_contract_id=filter_chunks_by_contract_id,\n",
    "        )\n",
    "        for a in agents\n",
    "    ]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    out = {r[\"agent_type\"]: r for r in results}\n",
    "    return out, time.perf_counter() - t0\n"
   ],
   "metadata": {
    "id": "c_q-vvH61e8l"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "def utc_now_iso() -> str:\n",
    "    \"\"\"Return current UTC timestamp in ISO 8601 format.\"\"\"\n",
    "    return datetime.now(timezone.utc).isoformat()\n"
   ],
   "metadata": {
    "id": "kVJXloNe2WmG"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Configure your run\n",
    "\n",
    "# - CONTRACT_ID can be any stable identifier you choose (used for memory persistence/recall).\n",
    "\n",
    "# - If your chunk vectors include contract_id in metadata, set FILTER_CHUNKS_BY_CONTRACT_ID=True.\n",
    "\n",
    "CONTRACT_ID = os.getenv(\"CONTRACT_ID\", \"demo_contract\")\n",
    "\n",
    "QUESTION = \"What are the payment terms, audit requirements, and uptime commitments?\"\n",
    "\n",
    "FILTER_CHUNKS_BY_CONTRACT_ID = False\n",
    "\n",
    "\n",
    "\n",
    "seq_out, seq_s = run_sequential(\n",
    "\n",
    "    question=QUESTION,\n",
    "\n",
    "    contract_id=CONTRACT_ID,\n",
    "\n",
    "    filter_chunks_by_contract_id=FILTER_CHUNKS_BY_CONTRACT_ID,\n",
    "\n",
    ")\n",
    "\n",
    "par_out, par_s = await run_parallel(\n",
    "\n",
    "    question=QUESTION,\n",
    "\n",
    "    contract_id=CONTRACT_ID,\n",
    "\n",
    "    filter_chunks_by_contract_id=FILTER_CHUNKS_BY_CONTRACT_ID,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Sequential seconds:\", round(seq_s, 3))\n",
    "\n",
    "print(\"Parallel seconds:  \", round(par_s, 3))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nPer-agent confidence (parallel):\")\n",
    "\n",
    "for a in AGENT_TYPES:\n",
    "\n",
    "    conf = par_out[a].get(\"confidence\")\n",
    "\n",
    "    print(f\"- {a}: {None if conf is None else round(conf, 4)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gi8D6Gxg2NTN",
    "outputId": "90ec6eca-0470-4879-ffc4-1b2f2e3bcc3a"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential seconds: 2.095\n",
      "Parallel seconds:   0.753\n",
      "\n",
      "Per-agent confidence (parallel):\n",
      "- legal_agent: 0.697\n",
      "- compliance_agent: 0.5864\n",
      "- finance_agent: 0.6143\n",
      "- operations_agent: 0.529\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "4) Persist Agent Outputs as Vector Memory (Pinecone)\n"
   ],
   "metadata": {
    "id": "7wh2g2iq2gkl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "import uuid\n",
    "import json\n"
   ],
   "metadata": {
    "id": "mZsz9hR82xmX"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ===============================\n",
    "# FULL AGENT MEMORY PERSIST CODE\n",
    "# ===============================\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime, timezone\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------\n",
    "CONTRACT_ID = \"contract_001\"\n",
    "QUESTION = \"Identify legal, compliance, finance and ops risks\"\n",
    "AGENT_TYPES = [\"legal\", \"compliance\", \"finance\", \"operations\"]\n",
    "AGENT_MEMORY_NAMESPACE = \"agent-memory\"\n",
    "\n",
    "# -------------------------------\n",
    "# MOCK PARALLEL AGENT OUTPUTS\n",
    "# (Replace with real pipeline output)\n",
    "# -------------------------------\n",
    "par_out = {\n",
    "    \"legal\": {\n",
    "        \"risks\": [\"Termination clause vague\", \"Unlimited liability\"],\n",
    "        \"severity\": \"high\"\n",
    "    },\n",
    "    \"compliance\": {\n",
    "        \"issues\": [\"GDPR consent missing\"],\n",
    "        \"severity\": \"medium\"\n",
    "    },\n",
    "    \"finance\": {\n",
    "        \"financial_risks\": [\"Late payment penalty 3% per month\"],\n",
    "        \"severity\": \"high\"\n",
    "    },\n",
    "    \"operations\": {\n",
    "        \"operational_risks\": [\"No SLA defined\"],\n",
    "        \"severity\": \"medium\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# DATA MODEL\n",
    "# -------------------------------\n",
    "@dataclass\n",
    "class AgentMemoryRecord:\n",
    "    contract_id: str\n",
    "    agent_type: str\n",
    "    timestamp: str\n",
    "    question: str\n",
    "    output: Dict[str, Any]\n",
    "\n",
    "# -------------------------------\n",
    "# UTILITIES\n",
    "# -------------------------------\n",
    "def utc_now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "# -------------------------------\n",
    "# PERSISTENCE LAYER\n",
    "# (DB / Pinecone stub \u2013 SAFE)\n",
    "# -------------------------------\n",
    "def persist_agent_memory(\n",
    "    records: List[AgentMemoryRecord],\n",
    "    namespace: str = \"agent-memory\"\n",
    ") -> List[str]:\n",
    "\n",
    "    ids = []\n",
    "\n",
    "    print(f\"\\n\ud83d\udce6 Persisting records into namespace: '{namespace}'\\n\")\n",
    "\n",
    "    for record in records:\n",
    "        vector_id = f\"{record.contract_id}-{record.agent_type}-{uuid.uuid4().hex[:8]}\"\n",
    "        payload = asdict(record)\n",
    "\n",
    "        # \ud83d\udd39 This is where Pinecone / DB upsert would go\n",
    "        # index.upsert(vectors=[(vector_id, embedding, payload)], namespace=namespace)\n",
    "\n",
    "        print(f\"\ud83e\udde0 Stored \u2192 {vector_id}\")\n",
    "        print(json.dumps(payload, indent=2))\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        ids.append(vector_id)\n",
    "\n",
    "    return ids\n",
    "\n",
    "# -------------------------------\n",
    "# BUILD RECORDS FROM PARALLEL OUTPUT\n",
    "# -------------------------------\n",
    "records = [\n",
    "    AgentMemoryRecord(\n",
    "        contract_id=CONTRACT_ID,\n",
    "        agent_type=agent,\n",
    "        timestamp=utc_now_iso(),\n",
    "        question=QUESTION,\n",
    "        output=par_out[agent]\n",
    "    )\n",
    "    for agent in AGENT_TYPES\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# PERSIST\n",
    "# -------------------------------\n",
    "ids = persist_agent_memory(\n",
    "    records=records,\n",
    "    namespace=AGENT_MEMORY_NAMESPACE\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2705 Upserted {len(ids)} agent-memory vectors\")\n",
    "print(\"\ud83d\udd11 Example IDs:\", ids[:2])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PVKXfNl8QeB8",
    "outputId": "74433d88-f6aa-4be6-9aa0-163455728ff5"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\ud83d\udce6 Persisting records into namespace: 'agent-memory'\n",
      "\n",
      "\ud83e\udde0 Stored \u2192 contract_001-legal-14daa9bc\n",
      "{\n",
      "  \"contract_id\": \"contract_001\",\n",
      "  \"agent_type\": \"legal\",\n",
      "  \"timestamp\": \"2026-01-19T12:36:50.520978+00:00\",\n",
      "  \"question\": \"Identify legal, compliance, finance and ops risks\",\n",
      "  \"output\": {\n",
      "    \"risks\": [\n",
      "      \"Termination clause vague\",\n",
      "      \"Unlimited liability\"\n",
      "    ],\n",
      "    \"severity\": \"high\"\n",
      "  }\n",
      "}\n",
      "------------------------------------------------------------\n",
      "\ud83e\udde0 Stored \u2192 contract_001-compliance-a1c26cf4\n",
      "{\n",
      "  \"contract_id\": \"contract_001\",\n",
      "  \"agent_type\": \"compliance\",\n",
      "  \"timestamp\": \"2026-01-19T12:36:50.520996+00:00\",\n",
      "  \"question\": \"Identify legal, compliance, finance and ops risks\",\n",
      "  \"output\": {\n",
      "    \"issues\": [\n",
      "      \"GDPR consent missing\"\n",
      "    ],\n",
      "    \"severity\": \"medium\"\n",
      "  }\n",
      "}\n",
      "------------------------------------------------------------\n",
      "\ud83e\udde0 Stored \u2192 contract_001-finance-9bb25e7e\n",
      "{\n",
      "  \"contract_id\": \"contract_001\",\n",
      "  \"agent_type\": \"finance\",\n",
      "  \"timestamp\": \"2026-01-19T12:36:50.521003+00:00\",\n",
      "  \"question\": \"Identify legal, compliance, finance and ops risks\",\n",
      "  \"output\": {\n",
      "    \"financial_risks\": [\n",
      "      \"Late payment penalty 3% per month\"\n",
      "    ],\n",
      "    \"severity\": \"high\"\n",
      "  }\n",
      "}\n",
      "------------------------------------------------------------\n",
      "\ud83e\udde0 Stored \u2192 contract_001-operations-85ad9742\n",
      "{\n",
      "  \"contract_id\": \"contract_001\",\n",
      "  \"agent_type\": \"operations\",\n",
      "  \"timestamp\": \"2026-01-19T12:36:50.521008+00:00\",\n",
      "  \"question\": \"Identify legal, compliance, finance and ops risks\",\n",
      "  \"output\": {\n",
      "    \"operational_risks\": [\n",
      "      \"No SLA defined\"\n",
      "    ],\n",
      "    \"severity\": \"medium\"\n",
      "  }\n",
      "}\n",
      "------------------------------------------------------------\n",
      "\n",
      "\u2705 Upserted 4 agent-memory vectors\n",
      "\ud83d\udd11 Example IDs: ['contract_001-legal-14daa9bc', 'contract_001-compliance-a1c26cf4']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "vfTajUhj2-JP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Recall Stored Agent Memory (No Rerun)\n"
   ],
   "metadata": {
    "id": "IMFBAiGq3BsV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Recall examples (filtered by contract_id and optionally agent_type)\n",
    "\n",
    "recall_ops = query_agent_memory(\n",
    "\n",
    "    query=\"uptime commitments service credits\",\n",
    "\n",
    "    contract_id=CONTRACT_ID,\n",
    "\n",
    "    agent_type=\"operations_agent\",\n",
    "\n",
    "    top_k=3,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "recall_fin = query_agent_memory(\n",
    "\n",
    "    query=\"interest charges late payment\",\n",
    "\n",
    "    contract_id=CONTRACT_ID,\n",
    "\n",
    "    agent_type=\"finance_agent\",\n",
    "\n",
    "    top_k=3,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Operations memory matches:\")\n",
    "\n",
    "for m in getattr(recall_ops, \"matches\", [])[:3]:\n",
    "\n",
    "    print(\"- score:\", getattr(m, \"score\", None))\n",
    "\n",
    "    print(\"  ts:\", (getattr(m, \"metadata\", {}) or {}).get(\"timestamp\"))\n",
    "\n",
    "    print(\"  question:\", (getattr(m, \"metadata\", {}) or {}).get(\"question\"))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nFinance memory matches:\")\n",
    "\n",
    "for m in getattr(recall_fin, \"matches\", [])[:3]:\n",
    "\n",
    "    print(\"- score:\", getattr(m, \"score\", None))\n",
    "\n",
    "    print(\"  ts:\", (getattr(m, \"metadata\", {}) or {}).get(\"timestamp\"))\n",
    "\n",
    "    print(\"  question:\", (getattr(m, \"metadata\", {}) or {}).get(\"question\"))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "A0XqcdA53CVj",
    "outputId": "1e455e1b-a83a-46d5-938c-a0d29e513b6c"
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'query_agent_memory' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1767635511.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Recall examples (filtered by contract_id and optionally agent_type)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m recall_ops = query_agent_memory(\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"uptime commitments service credits\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query_agent_memory' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# FULL AGENT MEMORY STORE + RECALL (SINGLE FILE)\n",
    "# ============================================================\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Any, List, Optional\n",
    "from datetime import datetime, timezone\n",
    "import uuid\n",
    "import json\n",
    "import math\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIG\n",
    "# ------------------------------------------------------------\n",
    "CONTRACT_ID = \"contract_001\"\n",
    "QUESTION = \"Identify legal, compliance, finance and ops risks\"\n",
    "\n",
    "AGENT_TYPES = [\n",
    "    \"legal_agent\",\n",
    "    \"compliance_agent\",\n",
    "    \"finance_agent\",\n",
    "    \"operations_agent\",\n",
    "]\n",
    "\n",
    "AGENT_MEMORY_NAMESPACE = \"agent-memory\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# IN-MEMORY VECTOR STORE (Pinecone stand-in)\n",
    "# ------------------------------------------------------------\n",
    "VECTOR_DB = []\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MOCK PARALLEL AGENT OUTPUTS\n",
    "# ------------------------------------------------------------\n",
    "par_out = {\n",
    "    \"legal_agent\": {\n",
    "        \"risks\": [\"Termination clause vague\", \"Unlimited liability\"]\n",
    "    },\n",
    "    \"compliance_agent\": {\n",
    "        \"issues\": [\"GDPR consent missing\"]\n",
    "    },\n",
    "    \"finance_agent\": {\n",
    "        \"financial_risks\": [\"Late payment interest 3% per month\"]\n",
    "    },\n",
    "    \"operations_agent\": {\n",
    "        \"operational_risks\": [\n",
    "            \"No SLA uptime commitment\",\n",
    "            \"No service credits defined\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# DATA MODELS\n",
    "# ------------------------------------------------------------\n",
    "@dataclass\n",
    "class AgentMemoryRecord:\n",
    "    contract_id: str\n",
    "    agent_type: str\n",
    "    timestamp: str\n",
    "    question: str\n",
    "    output: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class Match:\n",
    "    score: float\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class QueryResult:\n",
    "    matches: List[Match]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# UTILITIES\n",
    "# ------------------------------------------------------------\n",
    "def utc_now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def simple_text_embedding(text: str) -> set:\n",
    "    \"\"\"\n",
    "    Ultra-lightweight embedding:\n",
    "    converts text into a token set\n",
    "    \"\"\"\n",
    "    return set(text.lower().split())\n",
    "\n",
    "def cosine_sim(a: set, b: set) -> float:\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    return len(a & b) / math.sqrt(len(a) * len(b))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PERSIST MEMORY\n",
    "# ------------------------------------------------------------\n",
    "def persist_agent_memory(\n",
    "    records: List[AgentMemoryRecord],\n",
    "    namespace: str\n",
    ") -> List[str]:\n",
    "\n",
    "    ids = []\n",
    "\n",
    "    for r in records:\n",
    "        vector_id = f\"{r.contract_id}-{r.agent_type}-{uuid.uuid4().hex[:8]}\"\n",
    "        text_blob = json.dumps(r.output)\n",
    "\n",
    "        VECTOR_DB.append({\n",
    "            \"id\": vector_id,\n",
    "            \"namespace\": namespace,\n",
    "            \"embedding\": simple_text_embedding(text_blob),\n",
    "            \"metadata\": {\n",
    "                \"contract_id\": r.contract_id,\n",
    "                \"agent_type\": r.agent_type,\n",
    "                \"timestamp\": r.timestamp,\n",
    "                \"question\": r.question,\n",
    "                \"output\": r.output\n",
    "            }\n",
    "        })\n",
    "\n",
    "        ids.append(vector_id)\n",
    "\n",
    "    return ids\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# QUERY / RECALL MEMORY\n",
    "# ------------------------------------------------------------\n",
    "def query_agent_memory(\n",
    "    query: str,\n",
    "    contract_id: str,\n",
    "    agent_type: Optional[str] = None,\n",
    "    top_k: int = 3,\n",
    "    namespace: str = AGENT_MEMORY_NAMESPACE\n",
    ") -> QueryResult:\n",
    "\n",
    "    query_vec = simple_text_embedding(query)\n",
    "    scored = []\n",
    "\n",
    "    for item in VECTOR_DB:\n",
    "        meta = item[\"metadata\"]\n",
    "\n",
    "        if meta[\"contract_id\"] != contract_id:\n",
    "            continue\n",
    "\n",
    "        if agent_type and meta[\"agent_type\"] != agent_type:\n",
    "            continue\n",
    "\n",
    "        score = cosine_sim(query_vec, item[\"embedding\"])\n",
    "\n",
    "        if score > 0:\n",
    "            scored.append(\n",
    "                Match(\n",
    "                    score=round(score, 3),\n",
    "                    metadata=meta\n",
    "                )\n",
    "            )\n",
    "\n",
    "    scored.sort(key=lambda x: x.score, reverse=True)\n",
    "    return QueryResult(matches=scored[:top_k])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# BUILD & STORE PARALLEL OUTPUTS\n",
    "# ------------------------------------------------------------\n",
    "records = [\n",
    "    AgentMemoryRecord(\n",
    "        contract_id=CONTRACT_ID,\n",
    "        agent_type=a,\n",
    "        timestamp=utc_now_iso(),\n",
    "        question=QUESTION,\n",
    "        output=par_out[a],\n",
    "    )\n",
    "    for a in AGENT_TYPES\n",
    "]\n",
    "\n",
    "ids = persist_agent_memory(records, AGENT_MEMORY_NAMESPACE)\n",
    "\n",
    "print(f\"\u2705 Stored {len(ids)} agent-memory records\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# RECALL EXAMPLES\n",
    "# ------------------------------------------------------------\n",
    "recall_ops = query_agent_memory(\n",
    "    query=\"uptime commitments service credits\",\n",
    "    contract_id=CONTRACT_ID,\n",
    "    agent_type=\"operations_agent\",\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "recall_fin = query_agent_memory(\n",
    "    query=\"interest charges late payment\",\n",
    "    contract_id=CONTRACT_ID,\n",
    "    agent_type=\"finance_agent\",\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "print(\"Operations memory matches:\")\n",
    "for m in recall_ops.matches:\n",
    "    print(\"- score:\", m.score)\n",
    "    print(\"  ts:\", m.metadata.get(\"timestamp\"))\n",
    "    print(\"  question:\", m.metadata.get(\"question\"))\n",
    "\n",
    "print(\"\\nFinance memory matches:\")\n",
    "for m in recall_fin.matches:\n",
    "    print(\"- score:\", m.score)\n",
    "    print(\"  ts:\", m.metadata.get(\"timestamp\"))\n",
    "    print(\"  question:\", m.metadata.get(\"question\"))\n"
   ],
   "metadata": {
    "id": "ShYgR_eI3Eoz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "17d6ca61-2b6f-44b9-9099-cecf27d87a29"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Stored 4 agent-memory records\n",
      "\n",
      "Operations memory matches:\n",
      "- score: 0.5\n",
      "  ts: 2026-01-19T12:38:50.539590+00:00\n",
      "  question: Identify legal, compliance, finance and ops risks\n",
      "\n",
      "Finance memory matches:\n",
      "- score: 0.378\n",
      "  ts: 2026-01-19T12:38:50.539585+00:00\n",
      "  question: Identify legal, compliance, finance and ops risks\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pipeline / System Design Oriented\n",
    "\n",
    "Shared Context Refinement Pipeline\n",
    "\n",
    "Agent Memory Fusion & Iterative Refinement\n",
    "\n",
    "Collaborative Agent Refinement Workflow\n",
    "\n",
    "Context Aggregation \u2192 Refinement \u2192 Persistence\n",
    "> Add blockquote\n",
    "\n"
   ],
   "metadata": {
    "id": "Lt8Scw2N3IXz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "# Fallback if earlier cells were not executed yet\n",
    "AGENTS_FOR_REFINEMENT = globals().get(\"AGENT_TYPES\") or [\n",
    "    \"legal_agent\",\n",
    "    \"compliance_agent\",\n",
    "    \"finance_agent\",\n",
    "    \"operations_agent\",\n",
    "]\n",
    "\n",
    "def _as_utc_aware(dt: datetime) -> datetime:\n",
    "    \"\"\"Normalize datetimes to timezone-aware UTC for safe comparisons.\"\"\"\n",
    "    if dt.tzinfo is None:\n",
    "        return dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def _parse_ts(ts: Optional[str]) -> datetime:\n",
    "    # Always return a timezone-aware UTC datetime to avoid naive/aware comparison errors.\n",
    "    if not ts:\n",
    "        return datetime.min.replace(tzinfo=timezone.utc)\n",
    "    try:\n",
    "        # Handles ISO 8601 like: 2026-01-08T12:34:56.789+00:00 or ...Z\n",
    "        dt = datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "        return _as_utc_aware(dt)\n",
    "    except Exception:\n",
    "        return datetime.min.replace(tzinfo=timezone.utc)\n",
    "\n",
    "def _matches(resp: Any) -> List[Any]:\n",
    "    if isinstance(resp, dict):\n",
    "        return resp.get(\"matches\") or []\n",
    "    return getattr(resp, \"matches\", []) or []\n",
    "\n",
    "def _md(match: Any) -> Dict[str, Any]:\n",
    "    if isinstance(match, dict):\n",
    "        return match.get(\"metadata\") or {}\n",
    "    return getattr(match, \"metadata\", {}) or {}\n",
    "\n",
    "def _infer_risk_from_text(text: str) -> Tuple[str, str]:\n",
    "    t = (text or \"\").lower()\n",
    "    # Very simple heuristic just for milestone demonstration\n",
    "    high_terms = [\"penalt\", \"late fee\", \"interest\", \"termination\", \"breach\", \"indemn\", \"liability\", \"service credit\"]\n",
    "    medium_terms = [\"audit\", \"confidential\", \"privacy\", \"retention\", \"notification\", \"sla\"]\n",
    "\n",
    "    if any(k in t for k in high_terms):\n",
    "        return \"high\", \"Contains high-impact financial/legal terms (heuristic).\"\n",
    "    if any(k in t for k in medium_terms):\n",
    "        return \"medium\", \"Contains standard compliance/operations terms (heuristic).\"\n",
    "    return \"medium\", \"Defaulted to medium (insufficient signal in stored output).\"\n",
    "\n",
    "def fetch_latest_agent_memory(*, contract_id: str, agent_type: str, top_k: int = 10) -> Optional[Dict[str, Any]]:\n",
    "    resp = query_agent_memory(query=f\"{agent_type} risk assessment\", contract_id=contract_id, agent_type=agent_type, top_k=top_k)\n",
    "    best = None\n",
    "    best_ts = datetime.min.replace(tzinfo=timezone.utc)\n",
    "    for m in _matches(resp):\n",
    "        md = _md(m)\n",
    "        ts = _parse_ts(md.get(\"timestamp\"))\n",
    "        if ts > best_ts:\n",
    "            best = md\n",
    "            best_ts = ts\n",
    "    return best\n",
    "\n",
    "# 1) Retrieve latest memory per agent and build shared_context\n",
    "latest_by_agent: Dict[str, Dict[str, Any]] = {}\n",
    "for agent_type in AGENTS_FOR_REFINEMENT:\n",
    "    md = fetch_latest_agent_memory(contract_id=CONTRACT_ID, agent_type=agent_type)\n",
    "    if md is None:\n",
    "        latest_by_agent[agent_type] = {\n",
    "            \"agent\": agent_type,\n",
    "            \"risk_level\": \"unknown\",\n",
    "            \"confidence\": None,\n",
    "            \"timestamp\": None,\n",
    "            \"output_json\": \"\",\n",
    "        }\n",
    "        continue\n",
    "\n",
    "    # Prefer explicit risk_level metadata, else infer from stored output_json text\n",
    "    output_json = md.get(\"output_json\") or \"\"\n",
    "    risk_level = md.get(\"risk_level\")\n",
    "    if not isinstance(risk_level, str) or not risk_level.strip():\n",
    "        risk_level, _ = _infer_risk_from_text(output_json)\n",
    "\n",
    "    # Best-effort confidence from memory metadata\n",
    "    conf = md.get(\"confidence\")\n",
    "    if not isinstance(conf, (int, float)):\n",
    "        conf = None\n",
    "\n",
    "    latest_by_agent[agent_type] = {\n",
    "        \"agent\": md.get(\"agent\") or md.get(\"agent_type\") or agent_type,\n",
    "        \"risk_level\": risk_level,\n",
    "        \"confidence\": float(conf) if isinstance(conf, (int, float)) else None,\n",
    "        \"timestamp\": md.get(\"timestamp\"),\n",
    "        \"output_json\": output_json,\n",
    "    }\n",
    "\n",
    "shared_context = \"\\n\".join([f\"{v['agent']} risk: {v['risk_level']}\" for v in latest_by_agent.values()])\n",
    "print(shared_context)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h0twn6zq3IrN",
    "outputId": "57afb722-149f-4aa4-a4a4-253b7694bf66"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "legal_agent risk: unknown\n",
      "compliance_agent risk: unknown\n",
      "finance_agent risk: unknown\n",
      "operations_agent risk: unknown\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 2) Let compliance agent read finance output and refine (risk escalation demo)\n",
    "finance = latest_by_agent.get(\"finance_agent\", {})\n",
    "compliance = latest_by_agent.get(\"compliance_agent\", {})\n",
    "\n",
    "finance_risk = (finance.get(\"risk_level\") or \"unknown\").lower()\n",
    "compliance_risk = (compliance.get(\"risk_level\") or \"unknown\").lower()\n",
    "\n",
    "# Best-effort confidence: inherit from latest compliance if available, else finance, else None\n",
    "def _as_float(x: Any) -> Optional[float]:\n",
    "    return float(x) if isinstance(x, (int, float)) else None\n",
    "\n",
    "inherited_confidence = (\n",
    "    _as_float(compliance.get(\"confidence\"))\n",
    "    or _as_float((compliance.get(\"output\") or {}).get(\"confidence\") if isinstance(compliance.get(\"output\"), dict) else None)\n",
    "    or _as_float(finance.get(\"confidence\"))\n",
    "    or _as_float((finance.get(\"output\") or {}).get(\"confidence\") if isinstance(finance.get(\"output\"), dict) else None)\n",
    "    or None\n",
    " )\n",
    "\n",
    "refined_risk = compliance_risk\n",
    "reason = \"No escalation: finance risk not high (heuristic).\"\n",
    "\n",
    "if finance_risk == \"high\" and compliance_risk in {\"low\", \"medium\", \"unknown\"}:\n",
    "    refined_risk = \"high\"\n",
    "    reason = \"Escalated to high because finance risk is high; combined exposure increases compliance risk.\"\n",
    "\n",
    "refined_compliance = {\n",
    "    \"agent_type\": \"compliance_agent\",\n",
    "    \"risk_level\": refined_risk,\n",
    "    \"confidence\": inherited_confidence,\n",
    "    \"reason\": reason,\n",
    "    \"based_on\": {\n",
    "        \"shared_context\": shared_context,\n",
    "        \"finance_risk\": finance_risk,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(json.dumps(refined_compliance, indent=2))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-PhxHlC3Lri",
    "outputId": "ecced71a-b443-444c-dca3-15addc5c8158"
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n",
      "  \"agent_type\": \"compliance_agent\",\n",
      "  \"risk_level\": \"unknown\",\n",
      "  \"confidence\": null,\n",
      "  \"reason\": \"No escalation: finance risk not high (heuristic).\",\n",
      "  \"based_on\": {\n",
      "    \"shared_context\": \"legal_agent risk: unknown\\ncompliance_agent risk: unknown\\nfinance_agent risk: unknown\\noperations_agent risk: unknown\",\n",
      "    \"finance_risk\": \"unknown\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 3) Update Compliance memory (persist refined assessment)\n",
    "refined_record = AgentMemoryRecord(\n",
    "    contract_id=CONTRACT_ID,\n",
    "    agent_type=\"compliance_agent\",\n",
    "    timestamp=utc_now_iso(),\n",
    "    question=\"Cross-agent refinement: compliance reads finance output and re-evaluates risk\",\n",
    "    output=refined_compliance,\n",
    ")\n",
    "\n",
    "refined_ids = persist_agent_memory(records=[refined_record])\n",
    "print(\"Upserted refined compliance memory:\", refined_ids[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "LqFSU4sQ3PKE",
    "outputId": "b6542d41-d571-49a7-e720-e034c8dc8b90"
   },
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "persist_agent_memory() missing 1 required positional argument: 'namespace'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3920184117.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrefined_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersist_agent_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrefined_record\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Upserted refined compliance memory:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefined_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: persist_agent_memory() missing 1 required positional argument: 'namespace'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "7) Final Contract-Level JSON Output (Latest Memories \u2192 Standard JSON)"
   ],
   "metadata": {
    "id": "saRu76X28e6V"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the final schema + generate the final contract-level JSON from latest Pinecone memories\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "# =========================\n",
    "# \ud83d\udd39 OUTPUT PATH (FIXED)\n",
    "# =========================\n",
    "OUTPUTS_DIR = Path(\"/content/drive/MyDrive/ClauseAI/data/milestone3\")\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RISK_ORDER = {\"low\": 0, \"medium\": 1, \"high\": 2, \"unknown\": 1}\n",
    "HIGH_RISK_TERMS = [\n",
    "    \"penalt\", \"late fee\", \"interest\", \"termination\", \"breach\", \"indemn\", \"liability\", \"service credit\",\n",
    "    \"audit right\", \"uncapped\", \"limitation of liability\", \"data breach\", \"incident\", \"non-compliance\",\n",
    "    \"security\", \"subprocessor\", \"cross-border\", \"governing law\", \"injunction\",\n",
    "]\n",
    "\n",
    "# Defensive: if this list ever becomes nested\n",
    "if HIGH_RISK_TERMS and isinstance(HIGH_RISK_TERMS[0], (list, tuple, set)):\n",
    "    HIGH_RISK_TERMS = [t for group in HIGH_RISK_TERMS for t in group]\n",
    "\n",
    "FINAL_CONTRACT_SCHEMA: Dict[str, Any] = {\n",
    "    \"contract_id\": \"\",\n",
    "    \"legal\": {},\n",
    "    \"compliance\": {},\n",
    "    \"finance\": {},\n",
    "    \"operations\": {},\n",
    "    \"overall_risk\": \"\",\n",
    "    \"confidence\": {\n",
    "        \"per_agent\": {},\n",
    "        \"overall_avg\": None,\n",
    "    },\n",
    "    \"high_risk_clauses\": [],\n",
    "    \"generated_at\": \"\",\n",
    "}\n",
    "\n",
    "def _utc_now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def _as_utc_aware(dt: datetime) -> datetime:\n",
    "    if dt.tzinfo is None:\n",
    "        return dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def _parse_ts(ts: Optional[str]) -> datetime:\n",
    "    if not ts:\n",
    "        return datetime.min.replace(tzinfo=timezone.utc)\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "        return _as_utc_aware(dt)\n",
    "    except Exception:\n",
    "        return datetime.min.replace(tzinfo=timezone.utc)\n",
    "\n",
    "def _matches(resp: Any) -> List[Any]:\n",
    "    if isinstance(resp, dict):\n",
    "        return resp.get(\"matches\") or []\n",
    "    return getattr(resp, \"matches\", []) or []\n",
    "\n",
    "def _md(match: Any) -> Dict[str, Any]:\n",
    "    if isinstance(match, dict):\n",
    "        return match.get(\"metadata\") or {}\n",
    "    return getattr(match, \"metadata\", {}) or {}\n",
    "\n",
    "def _safe_json_loads(s: str) -> Optional[Any]:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _extract_text_from_match_metadata(md: Dict[str, Any]) -> str:\n",
    "    for key in (\"text\", \"chunk_text\", \"content\", \"clause_text\", \"snippet\", \"page_content\"):\n",
    "        v = md.get(key)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip()\n",
    "    try:\n",
    "        return json.dumps(md, ensure_ascii=False)[:800]\n",
    "    except Exception:\n",
    "        return str(md)[:800]\n",
    "\n",
    "def _infer_risk_level(output: Any) -> str:\n",
    "    if isinstance(output, dict):\n",
    "        rl = output.get(\"risk_level\")\n",
    "        if isinstance(rl, str) and rl.strip():\n",
    "            return rl.strip().lower()\n",
    "    text = json.dumps(output, ensure_ascii=False).lower() if output else \"\"\n",
    "    if any(t in text for t in HIGH_RISK_TERMS):\n",
    "        return \"high\"\n",
    "    return \"medium\"\n",
    "\n",
    "def _extract_term_hits(*, text: str, agent_type: str, max_items: int = 5) -> List[Dict[str, Any]]:\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return []\n",
    "    lower = text.lower()\n",
    "    out = []\n",
    "    seen_terms = set()\n",
    "\n",
    "    for term in HIGH_RISK_TERMS:\n",
    "        if term in seen_terms:\n",
    "            continue\n",
    "        idx = lower.find(term)\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        seen_terms.add(term)\n",
    "        snippet = text[max(0, idx - 120): idx + 160].replace(\"\\n\", \" \")\n",
    "        out.append({\n",
    "            \"agent\": agent_type,\n",
    "            \"query\": \"(memory-text-scan)\",\n",
    "            \"score\": None,\n",
    "            \"snippet\": snippet[:800],\n",
    "            \"is_high_risk\": True,\n",
    "            \"matched_term\": term,\n",
    "        })\n",
    "        if len(out) >= max_items:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def get_latest_agent_output(*, contract_id: str, agent_type: str, top_k: int = 10) -> Dict[str, Any]:\n",
    "    resp = query_agent_memory(\n",
    "        query=\"risk\",\n",
    "        contract_id=contract_id,\n",
    "        agent_type=agent_type,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    matches = _matches(resp)\n",
    "    if not matches:\n",
    "        return {\n",
    "            \"agent_type\": agent_type,\n",
    "            \"timestamp\": None,\n",
    "            \"risk_level\": \"unknown\",\n",
    "            \"confidence\": None,\n",
    "            \"output\": {\"risk_level\": \"unknown\", \"note\": \"No memory found\"},\n",
    "            \"_memory_metadata\": {},\n",
    "        }\n",
    "\n",
    "    ranked = [(_parse_ts(_md(m).get(\"timestamp\")), _md(m)) for m in matches]\n",
    "    ranked.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    best_md = ranked[0][1]\n",
    "    output_obj = _safe_json_loads(best_md.get(\"output_json\") or \"\") or {\n",
    "        \"raw_output_json\": best_md.get(\"output_json\")\n",
    "    }\n",
    "\n",
    "    risk_level = best_md.get(\"risk_level\") or _infer_risk_level(output_obj)\n",
    "\n",
    "    confidence = best_md.get(\"confidence\")\n",
    "    if not isinstance(confidence, (int, float)):\n",
    "        if isinstance(output_obj, dict):\n",
    "            confidence = output_obj.get(\"confidence\")\n",
    "\n",
    "    if not isinstance(confidence, (int, float)):\n",
    "        for _, md in ranked[1:]:\n",
    "            c = md.get(\"confidence\")\n",
    "            if isinstance(c, (int, float)):\n",
    "                confidence = c\n",
    "                break\n",
    "\n",
    "    return {\n",
    "        \"agent_type\": agent_type,\n",
    "        \"timestamp\": best_md.get(\"timestamp\"),\n",
    "        \"risk_level\": str(risk_level).lower(),\n",
    "        \"confidence\": confidence,\n",
    "        \"output\": output_obj,\n",
    "        \"_memory_metadata\": best_md,\n",
    "    }\n",
    "\n",
    "def _overall_risk(agent_risks: Dict[str, str]) -> str:\n",
    "    best = \"low\"\n",
    "    for r in agent_risks.values():\n",
    "        if RISK_ORDER.get(r, 1) > RISK_ORDER.get(best, 0):\n",
    "            best = r\n",
    "    return best if best in {\"low\", \"medium\", \"high\"} else \"medium\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# \ud83d\udd39 BUILD FINAL OUTPUT\n",
    "# =========================\n",
    "latest = {a: get_latest_agent_output(contract_id=CONTRACT_ID, agent_type=a) for a in AGENT_TYPES}\n",
    "\n",
    "final = dict(FINAL_CONTRACT_SCHEMA)\n",
    "final[\"contract_id\"] = CONTRACT_ID\n",
    "final[\"generated_at\"] = _utc_now_iso()\n",
    "\n",
    "agent_risks = {}\n",
    "conf_per_agent = {}\n",
    "high_risk_clauses = []\n",
    "\n",
    "for a in AGENT_TYPES:\n",
    "    payload = latest[a]\n",
    "    output_obj = payload[\"output\"]\n",
    "    agent_risks[a] = payload[\"risk_level\"]\n",
    "    conf_per_agent[a] = payload[\"confidence\"]\n",
    "\n",
    "    if not high_risk_clauses:\n",
    "        mem_text = payload[\"_memory_metadata\"].get(\"output_json\", \"\")\n",
    "        high_risk_clauses.extend(_extract_term_hits(text=mem_text, agent_type=a))\n",
    "\n",
    "final[\"legal\"] = latest[\"legal_agent\"][\"output\"]\n",
    "final[\"compliance\"] = latest[\"compliance_agent\"][\"output\"]\n",
    "final[\"finance\"] = latest[\"finance_agent\"][\"output\"]\n",
    "final[\"operations\"] = latest[\"operations_agent\"][\"output\"]\n",
    "\n",
    "final[\"overall_risk\"] = _overall_risk(agent_risks)\n",
    "final[\"confidence\"][\"per_agent\"] = conf_per_agent\n",
    "vals = [v for v in conf_per_agent.values() if isinstance(v, (int, float))]\n",
    "final[\"confidence\"][\"overall_avg\"] = sum(vals) / len(vals) if vals else None\n",
    "final[\"high_risk_clauses\"] = high_risk_clauses[:20]\n",
    "\n",
    "out_path = OUTPUTS_DIR / f\"final_contract_{CONTRACT_ID}.json\"\n",
    "out_path.write_text(json.dumps(final, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\u2705 Saved:\", out_path)\n",
    "print(\"Overall Risk:\", final[\"overall_risk\"])\n",
    "print(\"Confidence Avg:\", final[\"confidence\"][\"overall_avg\"])\n",
    "print(\"High-risk clauses:\", len(final[\"high_risk_clauses\"]))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JN4TaQbu3SYd",
    "outputId": "c003a67c-2f5e-4551-e0e6-d4660449891f"
   },
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Saved: /content/drive/MyDrive/ClauseAI/data/milestone3/final_contract_contract_001.json\n",
      "Overall Risk: medium\n",
      "Confidence Avg: None\n",
      "High-risk clauses: 0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "_DPQ2ZAx8kTP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Report Template Design (Human-Readable Output)**"
   ],
   "metadata": {
    "id": "uys_ZD0O8sQF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "save output of above"
   ],
   "metadata": {
    "id": "9Etz1VW6QSfP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Use a custom, empty mount point\n",
    "drive.mount(\"/content/my_drive\", force_remount=True)\n",
    "\n",
    "# Now your Drive is accessible at /content/my_drive/MyDrive/\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MxGhJjl0QzqO",
    "outputId": "11866955-78a1-4816-9e5d-bada9d51b454"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/my_drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 0\ufe0f\u20e3 Mount Google Drive\n",
    "# ============================================================\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "# Use a fresh mount point to avoid errors\n",
    "drive.mount(\"/content/my_drive\", force_remount=True)\n",
    "\n",
    "OUTPUT_DIR = Path(\"/content/my_drive/MyDrive/ClauseAI/data/milestone3\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# 1\ufe0f\u20e3 Report Template Code (your existing code)\n",
    "# ============================================================\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "REPORT_STRUCTURE = [\n",
    "    \"Executive Summary\",\n",
    "    \"Overall Risk Assessment\",\n",
    "    \"Legal Analysis\",\n",
    "    \"Compliance Analysis\",\n",
    "    \"Financial Analysis\",\n",
    "    \"Operational Analysis\",\n",
    "    \"Conclusion & Recommendations\",\n",
    "]\n",
    "\n",
    "def _bulletize(lines: List[str]) -> str:\n",
    "    return \"\\n\".join([f\"- {ln}\" for ln in lines if isinstance(ln, str) and ln.strip()])\n",
    "\n",
    "def _clean_snippet(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.replace(\"\\\\n\", \" \").replace(\"\\n\", \" \")\n",
    "    s = s.replace(\"\\\\\\\"\", '\"').replace(\"\\\\/\", \"/\")\n",
    "    s = \" \".join(s.split())\n",
    "    return s.strip()\n",
    "\n",
    "def _looks_like_json_fragment(s: str) -> bool:\n",
    "    if not isinstance(s, str):\n",
    "        return True\n",
    "    t = s.strip()\n",
    "    if not t:\n",
    "        return True\n",
    "    tl = t.lower()\n",
    "    hard_markers = [\n",
    "        \"top_k_per_query\", \"filter_chunks_by_contract_id\", \"per_query\", \"matches\",\n",
    "        \"retrieval\\\":\", \"metadata\\\":\", \"output_json\", \"raw_output_json\"\n",
    "    ]\n",
    "    if any(m in tl for m in hard_markers):\n",
    "        return True\n",
    "    jsonish = sum(t.count(ch) for ch in [\"{\", \"}\", \"[\", \"]\", \":\"])\n",
    "    backslashes = t.count(\"\\\\\")\n",
    "    quotes = t.count('\"')\n",
    "    letters = sum(ch.isalpha() for ch in t)\n",
    "    spaces = t.count(\" \")\n",
    "    if (jsonish + backslashes + quotes) >= 8 and (letters < 60 or spaces < 8):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def build_executive_summary(final_json: Dict[str, Any]) -> str:\n",
    "    risk = (final_json.get(\"overall_risk\") or \"medium\").lower()\n",
    "    conf = final_json.get(\"confidence\", {}).get(\"overall_avg\")\n",
    "    conf_s = \"unknown\" if conf is None else f\"{conf:.3f}\"\n",
    "    n_hi = len(final_json.get(\"high_risk_clauses\") or [])\n",
    "    return (\n",
    "        f\"Overall risk is {risk}. \"\n",
    "        f\"Confidence score average is {conf_s}. \"\n",
    "        f\"We found {n_hi} high-risk clause evidence snippets to review.\"\n",
    "    )\n",
    "\n",
    "def build_report(final_json: Dict[str, Any]) -> Dict[str, str]:\n",
    "    per_agent_conf = (final_json.get(\"confidence\") or {}).get(\"per_agent\") or {}\n",
    "    hi = final_json.get(\"high_risk_clauses\") or []\n",
    "\n",
    "    report: Dict[str, str] = {}\n",
    "    report[\"Executive Summary\"] = build_executive_summary(final_json)\n",
    "\n",
    "    report[\"Overall Risk Assessment\"] = _bulletize([\n",
    "        f\"Overall risk level: {(final_json.get('overall_risk') or 'medium').lower()}\",\n",
    "        f\"Confidence (avg): {final_json.get('confidence', {}).get('overall_avg')}\",\n",
    "        f\"Legal confidence: {per_agent_conf.get('legal_agent')}\",\n",
    "        f\"Compliance confidence: {per_agent_conf.get('compliance_agent')}\",\n",
    "        f\"Finance confidence: {per_agent_conf.get('finance_agent')}\",\n",
    "        f\"Operations confidence: {per_agent_conf.get('operations_agent')}\",\n",
    "    ])\n",
    "    report[\"Legal Analysis\"] = _bulletize([\n",
    "        \"Key legal obligations summarized from retrieval outputs.\",\n",
    "        \"Review termination, breach, and indemnity language if present.\",\n",
    "    ])\n",
    "    report[\"Compliance Analysis\"] = _bulletize([\n",
    "        \"Key privacy/security/compliance obligations summarized from retrieval outputs.\",\n",
    "        \"Review audit rights, incident notification, and data handling language if present.\",\n",
    "    ])\n",
    "    report[\"Financial Analysis\"] = _bulletize([\n",
    "        \"Key payment, invoicing, and late-fee obligations summarized from retrieval outputs.\",\n",
    "        \"Review liability and penalty exposure if present.\",\n",
    "    ])\n",
    "    report[\"Operational Analysis\"] = _bulletize([\n",
    "        \"Key deliverables, timelines, and SLA obligations summarized from retrieval outputs.\",\n",
    "        \"Review uptime commitments and service credits if present.\",\n",
    "    ])\n",
    "    top_evidence: List[str] = []\n",
    "    for item in hi:\n",
    "        if not isinstance(item, dict):\n",
    "            continue\n",
    "        raw = item.get(\"snippet\") or \"\"\n",
    "        snippet = _clean_snippet(raw)\n",
    "        if not snippet:\n",
    "            continue\n",
    "        if _looks_like_json_fragment(snippet):\n",
    "            continue\n",
    "        agent = item.get(\"agent\") or \"unknown_agent\"\n",
    "        term = item.get(\"matched_term\")\n",
    "        prefix = f\"[{agent}] \" + (f\"(term: {term}) \" if isinstance(term, str) and term else \"\")\n",
    "        top_evidence.append((prefix + snippet)[:220])\n",
    "        if len(top_evidence) >= 8:\n",
    "            break\n",
    "    report[\"Conclusion & Recommendations\"] = _bulletize([\n",
    "        \"Prioritize review of the high-risk clauses listed below.\",\n",
    "        \"If overall risk is high, consider negotiation points or approvals before signing.\",\n",
    "        *([\"High-risk evidence:\"] + top_evidence if top_evidence else [\"No clean high-risk evidence snippets were extracted from memory.\"]),\n",
    "    ])\n",
    "    return report\n",
    "\n",
    "# ============================================================\n",
    "# 2\ufe0f\u20e3 Demo final_json\n",
    "# ============================================================\n",
    "# If you don't have real final_json, you can use a dummy for testing\n",
    "final = {\n",
    "    \"overall_risk\": \"high\",\n",
    "    \"confidence\": {\"overall_avg\": 0.82, \"per_agent\": {\"legal_agent\": 0.9,\"compliance_agent\":0.8,\"finance_agent\":0.85,\"operations_agent\":0.75}},\n",
    "    \"high_risk_clauses\": [\n",
    "        {\"snippet\":\"This contract includes a penalty clause for late payment.\",\"agent\":\"legal_agent\",\"matched_term\":\"penalty\"},\n",
    "        {\"snippet\":\"Data breach notification requirements must be followed.\",\"agent\":\"compliance_agent\",\"matched_term\":\"data breach\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "report = build_report(final)\n",
    "\n",
    "# ============================================================\n",
    "# 3\ufe0f\u20e3 Print preview\n",
    "# ============================================================\n",
    "print(\"\\n\".join([\"=\" * 80, \"REPORT PREVIEW\", \"=\" * 80]))\n",
    "for section in REPORT_STRUCTURE:\n",
    "    print(f\"\\n## {section}\\n\")\n",
    "    print(report.get(section, \"\"))\n",
    "\n",
    "# ============================================================\n",
    "# 4\ufe0f\u20e3 SAVE OUTPUT TO DRIVE\n",
    "# ============================================================\n",
    "output_file = OUTPUT_DIR / \"report_output.txt\"\n",
    "output_file.write_text(\n",
    "    \"\\n\".join(f\"\\n## {section}\\n{report.get(section,'')}\" for section in REPORT_STRUCTURE),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2705 Report saved to: {output_file}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DqREziRVQU42",
    "outputId": "69c7ae8d-6a02-49d8-b708-3ef927b021a0"
   },
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/my_drive\n",
      "================================================================================\n",
      "REPORT PREVIEW\n",
      "================================================================================\n",
      "\n",
      "## Executive Summary\n",
      "\n",
      "Overall risk is high. Confidence score average is 0.820. We found 2 high-risk clause evidence snippets to review.\n",
      "\n",
      "## Overall Risk Assessment\n",
      "\n",
      "- Overall risk level: high\n",
      "- Confidence (avg): 0.82\n",
      "- Legal confidence: 0.9\n",
      "- Compliance confidence: 0.8\n",
      "- Finance confidence: 0.85\n",
      "- Operations confidence: 0.75\n",
      "\n",
      "## Legal Analysis\n",
      "\n",
      "- Key legal obligations summarized from retrieval outputs.\n",
      "- Review termination, breach, and indemnity language if present.\n",
      "\n",
      "## Compliance Analysis\n",
      "\n",
      "- Key privacy/security/compliance obligations summarized from retrieval outputs.\n",
      "- Review audit rights, incident notification, and data handling language if present.\n",
      "\n",
      "## Financial Analysis\n",
      "\n",
      "- Key payment, invoicing, and late-fee obligations summarized from retrieval outputs.\n",
      "- Review liability and penalty exposure if present.\n",
      "\n",
      "## Operational Analysis\n",
      "\n",
      "- Key deliverables, timelines, and SLA obligations summarized from retrieval outputs.\n",
      "- Review uptime commitments and service credits if present.\n",
      "\n",
      "## Conclusion & Recommendations\n",
      "\n",
      "- Prioritize review of the high-risk clauses listed below.\n",
      "- If overall risk is high, consider negotiation points or approvals before signing.\n",
      "- High-risk evidence:\n",
      "- [legal_agent] (term: penalty) This contract includes a penalty clause for late payment.\n",
      "- [compliance_agent] (term: data breach) Data breach notification requirements must be followed.\n",
      "\n",
      "\u2705 Report saved to: /content/my_drive/MyDrive/ClauseAI/data/milestone3/report_output.txt\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Report Formatting & Tone Customization**"
   ],
   "metadata": {
    "id": "vEydB8lqK7LT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Report Formatting & Tone Customization\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "# ----------------------------\n",
    "# 0\ufe0f\u20e3 Input file (saved report)\n",
    "# ----------------------------\n",
    "INPUT_FILE = Path(\"/content/my_drive/MyDrive/ClauseAI/data/milestone3/report_output.txt\")\n",
    "\n",
    "if not INPUT_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Report file not found: {INPUT_FILE}\")\n",
    "\n",
    "# Read the report sections from file\n",
    "raw_report_text = INPUT_FILE.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# Split into sections (assuming ## SECTION_NAME format)\n",
    "sections_raw = raw_report_text.split(\"\\n## \")\n",
    "report_dict: Dict[str, str] = {}\n",
    "\n",
    "for sec in sections_raw:\n",
    "    if not sec.strip():\n",
    "        continue\n",
    "    lines = sec.strip().split(\"\\n\")\n",
    "    title = lines[0].strip()\n",
    "    body = \"\\n\".join(lines[1:]).strip()\n",
    "    report_dict[title] = body\n",
    "\n",
    "# ----------------------------\n",
    "# 1\ufe0f\u20e3 Tone Templates\n",
    "# ----------------------------\n",
    "TONE_TEMPLATES = {\n",
    "    \"neutral\": {\n",
    "        \"header_prefix\": \"\",\n",
    "        \"risk_marker\": \"\u26a0\ufe0f\",\n",
    "        \"style_note\": \"Neutral factual tone\",\n",
    "    },\n",
    "    \"executive\": {\n",
    "        \"header_prefix\": \"\ud83d\udd39 \",\n",
    "        \"risk_marker\": \"\ud83d\udea8\",\n",
    "        \"style_note\": \"Concise executive summary tone\",\n",
    "    },\n",
    "    \"legal\": {\n",
    "        \"header_prefix\": \"\u00a7 \",\n",
    "        \"risk_marker\": \"\u2757\",\n",
    "        \"style_note\": \"Formal legal-review tone\",\n",
    "    },\n",
    "}\n",
    "\n",
    "HIGH_RISK_SECTIONS = {\n",
    "    \"Overall Risk Assessment\",\n",
    "    \"Legal Analysis\",\n",
    "    \"Compliance Analysis\",\n",
    "    \"Financial Analysis\",\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 2\ufe0f\u20e3 Section Formatter\n",
    "# ----------------------------\n",
    "def format_section(title: str, content: str, tone: str = \"neutral\") -> str:\n",
    "    cfg = TONE_TEMPLATES[tone]\n",
    "    header = f\"{cfg['header_prefix']}## {title}\"\n",
    "\n",
    "    if title in HIGH_RISK_SECTIONS:\n",
    "        header += f\"  {cfg['risk_marker']} HIGH ATTENTION\"\n",
    "\n",
    "    # Force bullet points\n",
    "    lines = []\n",
    "    for ln in content.split(\"\\n\"):\n",
    "        ln = ln.strip()\n",
    "        if not ln:\n",
    "            continue\n",
    "        if not ln.startswith(\"-\"):\n",
    "            ln = f\"- {ln}\"\n",
    "        lines.append(ln)\n",
    "\n",
    "    body = \"\\n\".join(lines) if lines else \"- No notable findings.\"\n",
    "    return f\"{header}\\n{body}\\n\"\n",
    "\n",
    "# ----------------------------\n",
    "# 3\ufe0f\u20e3 Format Entire Report\n",
    "# ----------------------------\n",
    "def format_full_report(\n",
    "    report: Dict[str, str],\n",
    "    tone: str = \"neutral\"\n",
    ") -> str:\n",
    "    if tone not in TONE_TEMPLATES:\n",
    "        raise ValueError(f\"Unsupported tone: {tone}\")\n",
    "\n",
    "    output = []\n",
    "    output.append(f\"# Contract Review Report ({tone.upper()})\")\n",
    "    output.append(f\"_Style: {TONE_TEMPLATES[tone]['style_note']}_\\n\")\n",
    "\n",
    "    for section, content in report.items():\n",
    "        output.append(format_section(section, content, tone))\n",
    "\n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "# ----------------------------\n",
    "# 4\ufe0f\u20e3 Generate Executive Report (short version)\n",
    "# ----------------------------\n",
    "def generate_executive_report(report: Dict[str, str], tone: str = \"executive\") -> str:\n",
    "    key_sections = [\n",
    "        \"Executive Summary\",\n",
    "        \"Overall Risk Assessment\",\n",
    "        \"Conclusion & Recommendations\",\n",
    "    ]\n",
    "    output = [\"# Executive Contract Summary\\n\"]\n",
    "    for sec in key_sections:\n",
    "        content = report.get(sec, \"\")\n",
    "        output.append(format_section(sec, content, tone))\n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "# ----------------------------\n",
    "# 5\ufe0f\u20e3 USAGE\n",
    "# ----------------------------\n",
    "\n",
    "# Full reports with different tones\n",
    "neutral_report = format_full_report(report_dict, tone=\"neutral\")\n",
    "executive_report = format_full_report(report_dict, tone=\"executive\")\n",
    "legal_report = format_full_report(report_dict, tone=\"legal\")\n",
    "\n",
    "# Executive-only short version\n",
    "executive_summary_only = generate_executive_report(report_dict, tone=\"executive\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6\ufe0f\u20e3 SAVE FORMATTED REPORTS\n",
    "# ----------------------------\n",
    "OUTPUT_DIR = INPUT_FILE.parent\n",
    "(OUTPUT_DIR / \"formatted_reports\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save\n",
    "(OUTPUT_DIR / \"formatted_reports/report_neutral.txt\").write_text(neutral_report, encoding=\"utf-8\")\n",
    "(OUTPUT_DIR / \"formatted_reports/report_executive.txt\").write_text(executive_report, encoding=\"utf-8\")\n",
    "(OUTPUT_DIR / \"formatted_reports/report_legal.txt\").write_text(legal_report, encoding=\"utf-8\")\n",
    "(OUTPUT_DIR / \"formatted_reports/report_executive_summary.txt\").write_text(executive_summary_only, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\u2705 Formatted reports saved in: {OUTPUT_DIR / 'formatted_reports'}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00phIA_LSjN4",
    "outputId": "71bbd04b-5ecb-4178-fc1e-cf1f1f668ca8"
   },
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Formatted reports saved in: /content/my_drive/MyDrive/ClauseAI/data/milestone3/formatted_reports\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**FastAPI Backend for Contract Analysis**"
   ],
   "metadata": {
    "id": "TvnMlTYBSjhM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# FastAPI Contract Analysis Backend (Colab + Ngrok) \u2014 STABLE\n",
    "# ============================================================\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Install compatible versions\n",
    "# -----------------------------\n",
    "!pip install -q fastapi==0.110.0 uvicorn==0.29.0 pyngrok nest-asyncio python-multipart\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Imports\n",
    "# -----------------------------\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File, Form\n",
    "from fastapi.responses import JSONResponse\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Patch asyncio (Colab-safe)\n",
    "# -----------------------------\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Ngrok Auth\n",
    "# -----------------------------\n",
    "NGROK_AUTH_TOKEN = \"38T4vHLNLquFXmIzfD0fPZHkxkJ_5ce2KBYrRKDXxsSxLrJVm\"\n",
    "!ngrok authtoken {NGROK_AUTH_TOKEN}\n",
    "\n",
    "# Kill old tunnels to avoid ERR_NGROK_324\n",
    "ngrok.kill()\n",
    "print(\"\u2705 Old ngrok tunnels killed\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. FastAPI App\n",
    "# -----------------------------\n",
    "app = FastAPI(title=\"Contract Analysis API\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Report Utilities\n",
    "# -----------------------------\n",
    "def _bulletize(lines: List[str]) -> str:\n",
    "    return \"\\n\".join(f\"- {l}\" for l in lines if l)\n",
    "\n",
    "def build_report(final_json: Dict[str, Any], tone: str = \"executive\") -> Dict[str, str]:\n",
    "    risk = final_json.get(\"overall_risk\", \"medium\")\n",
    "    hi = final_json.get(\"high_risk_clauses\", [])\n",
    "\n",
    "    return {\n",
    "        \"Executive Summary\": (\n",
    "            f\"Overall contract risk is **{risk.upper()}**. \"\n",
    "            f\"{len(hi)} high-risk clauses require review.\"\n",
    "        ),\n",
    "        \"Overall Risk Assessment\": _bulletize([\n",
    "            f\"Risk level: {risk}\",\n",
    "            f\"High-risk clauses detected: {len(hi)}\",\n",
    "        ]),\n",
    "        \"Legal Analysis\": _bulletize([\n",
    "            \"Review termination, liability, indemnity clauses.\"\n",
    "        ]),\n",
    "        \"Compliance Analysis\": _bulletize([\n",
    "            \"Review data protection, audit, and regulatory clauses.\"\n",
    "        ]),\n",
    "        \"Financial Analysis\": _bulletize([\n",
    "            \"Review payment terms, penalties, and interest clauses.\"\n",
    "        ]),\n",
    "        \"Operational Analysis\": _bulletize([\n",
    "            \"Review SLA, uptime, service credits.\"\n",
    "        ]),\n",
    "        \"Conclusion & Recommendations\": _bulletize([\n",
    "            \"Prioritize high-risk clauses before signing.\",\n",
    "            \"Seek legal approval if risk is high.\"\n",
    "        ])\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# 7. API Endpoints\n",
    "# -----------------------------\n",
    "@app.post(\"/analyze\")\n",
    "async def analyze_contract(\n",
    "    file: UploadFile = File(...),\n",
    "    tone: str = Form(\"executive\")\n",
    "):\n",
    "    if not file.filename.endswith(\".json\"):\n",
    "        raise HTTPException(status_code=400, detail=\"Only JSON files allowed\")\n",
    "\n",
    "    content = await file.read()\n",
    "    if not content:\n",
    "        raise HTTPException(status_code=400, detail=\"Empty file uploaded\")\n",
    "\n",
    "    try:\n",
    "        final_json = json.loads(content)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=f\"Invalid JSON: {e}\")\n",
    "\n",
    "    return JSONResponse(build_report(final_json, tone))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"ok\"}\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Save app.py (REQUIRED)\n",
    "# -----------------------------\n",
    "with open(\"app.py\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "from fastapi import FastAPI\n",
    "from main import app\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Start Uvicorn via subprocess (FIX)\n",
    "# -----------------------------\n",
    "PORT = 8000\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    [\"uvicorn\", \"__main__:app\", \"--host\", \"0.0.0.0\", \"--port\", str(PORT)],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL\n",
    ")\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Start Ngrok\n",
    "# -----------------------------\n",
    "public_url = ngrok.connect(PORT)\n",
    "print(f\"\ud83d\udd17 Public URL: {public_url}\")\n",
    "print(\"\u2705 FastAPI server is LIVE\")\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8axY62pRthF",
    "outputId": "3a6dedde-f3d6-4b16-9ecd-a6540387d2fb"
   },
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/92.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sse-starlette 3.1.2 requires starlette>=0.49.1, but you have starlette 0.36.3 which is incompatible.\n",
      "google-adk 1.21.0 requires fastapi<0.124.0,>=0.115.0, but you have fastapi 0.110.0 which is incompatible.\n",
      "google-adk 1.21.0 requires starlette<1.0.0,>=0.49.1, but you have starlette 0.36.3 which is incompatible.\n",
      "google-adk 1.21.0 requires uvicorn<1.0.0,>=0.34.0, but you have uvicorn 0.29.0 which is incompatible.\n",
      "mcp 1.25.0 requires uvicorn>=0.31.1; sys_platform != \"emscripten\", but you have uvicorn 0.29.0 which is incompatible.\n",
      "gradio 5.50.0 requires fastapi<1.0,>=0.115.2, but you have fastapi 0.110.0 which is incompatible.\n",
      "gradio 5.50.0 requires starlette<1.0,>=0.40.0, but you have starlette 0.36.3 which is incompatible.\n",
      "python-fasthtml 0.12.37 requires uvicorn[standard]>=0.30, but you have uvicorn 0.29.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
      "\u2705 Old ngrok tunnels killed\n",
      "\ud83d\udd17 Public URL: NgrokTunnel: \"https://uncensured-boughten-quinn.ngrok-free.dev\" -> \"http://localhost:8000\"\n",
      "\u2705 FastAPI server is LIVE\n"
     ]
    }
   ]
  }
 ]
}